{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, Sequence, Callable\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "import augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "source": [
    "## 1. 커스텀 데이터셋 만들기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(image):\n",
    "    \"\"\"\n",
    "    hyperparms\n",
    "    \"\"\"\n",
    "    mixture_depth = -1\n",
    "    mixture_width = 3\n",
    "    aug_severity = 3\n",
    "\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(\n",
    "          [0.485, 0.456, 0.406],\n",
    "          [0.229, 0.224, 0.225])])\n",
    "\n",
    "    aug_list = augmentations.augmentations_all\n",
    "    ws = np.float32(np.random.dirichlet([1] * mixture_width))\n",
    "    m = np.float32(np.random.beta(1, 1))\n",
    "\n",
    "    mix = torch.zeros_like(preprocess(image))\n",
    "    for i in range(mixture_width):\n",
    "        image_aug = image.copy()\n",
    "        depth = mixture_depth if mixture_depth > 0 else np.random.randint(1, 4)\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            op = np.random.choice(aug_list)\n",
    "            image_aug = op(image_aug, aug_severity)\n",
    "            # Preprocessing commutes since all coefficients are convex\n",
    "        \n",
    "        #print(preprocess(image_aug))\n",
    "        mix += ws[i] * preprocess(image_aug)\n",
    "\n",
    "    mixed = (1 - m) * preprocess(image) + m * mix\n",
    "    return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dir: os.PathLike,\n",
    "        image_ids: os.PathLike,\n",
    "        transforms: Sequence[Callable]\n",
    "    ) -> None:\n",
    "        self.dir = dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.labels = {}\n",
    "        with open(image_ids, 'r') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for row in reader:\n",
    "                self.labels[int(row[0])] = list(map(int, row[1:]))\n",
    "\n",
    "        self.image_ids = list(self.labels.keys())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Tensor]:\n",
    "        image_id = self.image_ids[index]\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                self.dir, f'{str(image_id).zfill(5)}.png')).convert('RGB')\n",
    "        target = np.array(self.labels.get(image_id)).astype(np.float32)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return aug(image), target"
   ]
  },
  {
   "source": [
    "## 2. 이미지 어그멘테이션"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomVerticalFlip(p=0.5),\n",
    "    # transforms.RandomCrop(32, padding =4), # added random cropping\n",
    "    #transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transforms_test = transforms.Compose([\n",
    "    #transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done trainset\nsplit done\ndone testset\ndone train loader\ndone valid loader\ndone test laoder\n"
     ]
    }
   ],
   "source": [
    "trainset = MnistDataset('2nd_data/dirty_mnist_2nd', '2nd_data/dirty_mnist_2nd_answer.csv', transforms_train)\n",
    "print('done trainset')\n",
    "trainset, validset = torch.utils.data.random_split(trainset, [45000,5000])\n",
    "print('split done')\n",
    "\n",
    "testset = MnistDataset('2nd_data/test_dirty_mnist_2nd', '2nd_data/sample_submission.csv', transforms_test)\n",
    "print('done testset')\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=32, num_workers=8)\n",
    "print('done train loader')\n",
    "\n",
    "valid_loader = DataLoader(validset, batch_size = 32, num_workers = 4)\n",
    "print('done valid loader')\n",
    "test_loader = DataLoader(testset, batch_size=32, num_workers=4)\n",
    "print('done test laoder')"
   ]
  },
  {
   "source": [
    "## 3. ResNet50 모형"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─ResNet: 1-1                            [1, 1000]                 --\n|    └─Conv2d: 2-1                       [1, 64, 128, 128]         9,408\n|    └─BatchNorm2d: 2-2                  [1, 64, 128, 128]         128\n|    └─ReLU: 2-3                         [1, 64, 128, 128]         --\n|    └─MaxPool2d: 2-4                    [1, 64, 64, 64]           --\n|    └─Sequential: 2-5                   [1, 256, 64, 64]          --\n|    |    └─Bottleneck: 3-1              [1, 256, 64, 64]          75,008\n|    |    └─Bottleneck: 3-2              [1, 256, 64, 64]          70,400\n|    |    └─Bottleneck: 3-3              [1, 256, 64, 64]          70,400\n|    └─Sequential: 2-6                   [1, 512, 32, 32]          --\n|    |    └─Bottleneck: 3-4              [1, 512, 32, 32]          379,392\n|    |    └─Bottleneck: 3-5              [1, 512, 32, 32]          280,064\n|    |    └─Bottleneck: 3-6              [1, 512, 32, 32]          280,064\n|    |    └─Bottleneck: 3-7              [1, 512, 32, 32]          280,064\n|    └─Sequential: 2-7                   [1, 1024, 16, 16]         --\n|    |    └─Bottleneck: 3-8              [1, 1024, 16, 16]         1,512,448\n|    |    └─Bottleneck: 3-9              [1, 1024, 16, 16]         1,117,184\n|    |    └─Bottleneck: 3-10             [1, 1024, 16, 16]         1,117,184\n|    |    └─Bottleneck: 3-11             [1, 1024, 16, 16]         1,117,184\n|    |    └─Bottleneck: 3-12             [1, 1024, 16, 16]         1,117,184\n|    |    └─Bottleneck: 3-13             [1, 1024, 16, 16]         1,117,184\n|    └─Sequential: 2-8                   [1, 2048, 8, 8]           --\n|    |    └─Bottleneck: 3-14             [1, 2048, 8, 8]           6,039,552\n|    |    └─Bottleneck: 3-15             [1, 2048, 8, 8]           4,462,592\n|    |    └─Bottleneck: 3-16             [1, 2048, 8, 8]           4,462,592\n|    └─AdaptiveAvgPool2d: 2-9            [1, 2048, 1, 1]           --\n|    └─Linear: 2-10                      [1, 1000]                 2,049,000\n├─Linear: 1-2                            [1, 26]                   26,026\n==========================================================================================\nTotal params: 25,583,058\nTrainable params: 25,583,058\nNon-trainable params: 0\nTotal mult-adds (G): 4.95\n==========================================================================================\nInput size (MB): 0.79\nForward/backward pass size (MB): 200.81\nParams size (MB): 102.33\nEstimated Total Size (MB): 303.93\n==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.resnet = resnet50(pretrained=True)\n",
    "        self.classifier = nn.Linear(1000, 26)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MnistModel().to(device)\n",
    "print(summary(model, input_size=(1, 3, 256, 256), verbose=0))"
   ]
  },
  {
   "source": [
    "## 4. 학습하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
    "                            Default: 7\n",
    "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "            path (str): checkpoint저장 경로\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "46, acc: 62.50%\n",
      "[10] loss: 0.63448, acc: 59.50%\n",
      "[10] loss: 0.64323, acc: 60.10%\n",
      "[10] loss: 0.63104, acc: 58.17%\n",
      "[10] loss: 0.64320, acc: 56.61%\n",
      "[10] loss: 0.62963, acc: 59.01%\n",
      "[10] loss: 0.64552, acc: 58.77%\n",
      "[10] loss: 0.62020, acc: 60.22%\n",
      "[10] loss: 0.63975, acc: 59.62%\n",
      "[10] loss: 0.62763, acc: 60.58%\n",
      "[10] loss: 0.63846, acc: 60.22%\n",
      "[10] loss: 0.64224, acc: 59.38%\n",
      "[10] loss: 0.64977, acc: 58.89%\n",
      "[10] loss: 0.63112, acc: 62.14%\n",
      "[10] loss: 0.62974, acc: 61.06%\n",
      "[10] loss: 0.63517, acc: 58.53%\n",
      "[10] loss: 0.61973, acc: 61.90%\n",
      "[10] loss: 0.62662, acc: 61.66%\n",
      "[10] loss: 0.62565, acc: 61.78%\n",
      "[10] loss: 0.64589, acc: 59.98%\n",
      "[10] loss: 0.64185, acc: 60.58%\n",
      "[10] loss: 0.61668, acc: 61.42%\n",
      "[10] loss: 0.62657, acc: 62.62%\n",
      "[10] loss: 0.64867, acc: 60.70%\n",
      "[10] loss: 0.62735, acc: 61.66%\n",
      "[10] loss: 0.65563, acc: 59.01%\n",
      "[10] loss: 0.63166, acc: 59.62%\n",
      "[10] loss: 0.62067, acc: 59.86%\n",
      "[10] loss: 0.62978, acc: 61.54%\n",
      "[10] loss: 0.62123, acc: 62.86%\n",
      "[10] loss: 0.63770, acc: 60.70%\n",
      "[10] loss: 0.64027, acc: 57.81%\n",
      "[10] loss: 0.64764, acc: 58.53%\n",
      "[10] loss: 0.64496, acc: 60.10%\n",
      "[10] loss: 0.64657, acc: 58.41%\n",
      "[10] loss: 0.62766, acc: 59.98%\n",
      "[10] loss: 0.66233, acc: 58.77%\n",
      "[10] loss: 0.63246, acc: 61.78%\n",
      "[10] loss: 0.65749, acc: 58.41%\n",
      "[10] loss: 0.62141, acc: 60.22%\n",
      "[10] loss: 0.64411, acc: 59.98%\n",
      "[10] loss: 0.63047, acc: 58.89%\n",
      "[10] loss: 0.61402, acc: 61.06%\n",
      "[10] loss: 0.64989, acc: 56.73%\n",
      "[10] loss: 0.64328, acc: 60.94%\n",
      "[10] loss: 0.64831, acc: 58.17%\n",
      "[10] loss: 0.64466, acc: 58.41%\n",
      "[10] loss: 0.63955, acc: 59.98%\n",
      "[10] loss: 0.64064, acc: 60.70%\n",
      "[10] loss: 0.63508, acc: 60.46%\n",
      "[10] loss: 0.64382, acc: 60.34%\n",
      "[10] loss: 0.61745, acc: 62.26%\n",
      "[10] loss: 0.62560, acc: 61.66%\n",
      "[10] loss: 0.64502, acc: 59.25%\n",
      "[10] loss: 0.64475, acc: 56.97%\n",
      "[10] loss: 0.63768, acc: 60.10%\n",
      "[10] loss: 0.62053, acc: 60.34%\n",
      "[10] loss: 0.62403, acc: 59.50%\n",
      "[10] loss: 0.60727, acc: 62.02%\n",
      "[10] loss: 0.65600, acc: 58.65%\n",
      "[10] loss: 0.61009, acc: 62.50%\n",
      "[10] loss: 0.62635, acc: 60.58%\n",
      "[10] loss: 0.61574, acc: 59.62%\n",
      "[10] loss: 0.63939, acc: 60.46%\n",
      "[10] loss: 0.64586, acc: 58.89%\n",
      "[10] loss: 0.61362, acc: 61.30%\n",
      "[10] loss: 0.63319, acc: 60.70%\n",
      "[10] loss: 0.63167, acc: 60.82%\n",
      "[10] loss: 0.63439, acc: 58.77%\n",
      "[10] loss: 0.62427, acc: 61.18%\n",
      "[10] loss: 0.63995, acc: 60.70%\n",
      "[10] loss: 0.64325, acc: 62.26%\n",
      "[10] loss: 0.62588, acc: 60.70%\n",
      "[10] loss: 0.61761, acc: 60.10%\n",
      "[10] loss: 0.62154, acc: 61.30%\n",
      "[10] loss: 0.63061, acc: 60.10%\n",
      "[10] loss: 0.61729, acc: 60.70%\n",
      "[10] loss: 0.61535, acc: 63.34%\n",
      "[10] loss: 0.62314, acc: 61.90%\n",
      "[10] loss: 0.64387, acc: 59.74%\n",
      "[10] loss: 0.64724, acc: 59.13%\n",
      "[10] loss: 0.63857, acc: 59.98%\n",
      "[10] loss: 0.62826, acc: 59.74%\n",
      "[10] loss: 0.62955, acc: 59.74%\n",
      "[10] loss: 0.63855, acc: 60.22%\n",
      "[10] loss: 0.62832, acc: 60.70%\n",
      "[10] loss: 0.62753, acc: 61.18%\n",
      "[10] loss: 0.62596, acc: 60.94%\n",
      "[10] loss: 0.62757, acc: 61.54%\n",
      "[10] loss: 0.62331, acc: 62.02%\n",
      "[10] loss: 0.65019, acc: 60.46%\n",
      "[10] loss: 0.61791, acc: 61.06%\n",
      "[10] loss: 0.63539, acc: 59.98%\n",
      "[10] loss: 0.63584, acc: 60.58%\n",
      "[10] loss: 0.62889, acc: 59.62%\n",
      "[10] loss: 0.62941, acc: 60.94%\n",
      "[10] loss: 0.63460, acc: 59.98%\n",
      "[10] loss: 0.61614, acc: 63.22%\n",
      "[10] loss: 0.61815, acc: 59.62%\n",
      "[10] loss: 0.61297, acc: 61.54%\n",
      "[10] loss: 0.63379, acc: 59.98%\n",
      "[10] loss: 0.63172, acc: 61.66%\n",
      "[10] loss: 0.61793, acc: 60.22%\n",
      "[10] loss: 0.61004, acc: 61.78%\n",
      "[10] loss: 0.61806, acc: 60.94%\n",
      "[10] loss: 0.62129, acc: 60.70%\n",
      "[10] loss: 0.63237, acc: 61.42%\n",
      "[10] loss: 0.64301, acc: 60.70%\n",
      "[  10/1000] train_loss: 0.63318 valid_loss: 0.64486\n",
      "Validation loss decreased (0.656596 --> 0.644863).  Saving model ...\n",
      "========= Start Epoch 11 =========\n",
      "[11] loss: 0.60421, acc: 64.18%\n",
      "[11] loss: 0.62587, acc: 62.86%\n",
      "[11] loss: 0.60419, acc: 62.14%\n",
      "[11] loss: 0.62488, acc: 60.82%\n",
      "[11] loss: 0.64379, acc: 59.62%\n",
      "[11] loss: 0.64543, acc: 60.34%\n",
      "[11] loss: 0.60716, acc: 63.70%\n",
      "[11] loss: 0.61349, acc: 62.86%\n",
      "[11] loss: 0.61246, acc: 62.14%\n",
      "[11] loss: 0.63346, acc: 60.94%\n",
      "[11] loss: 0.64062, acc: 60.34%\n",
      "[11] loss: 0.65128, acc: 59.01%\n",
      "[11] loss: 0.62376, acc: 62.02%\n",
      "[11] loss: 0.62794, acc: 62.74%\n",
      "[11] loss: 0.63197, acc: 61.18%\n",
      "[11] loss: 0.62256, acc: 61.90%\n",
      "[11] loss: 0.63326, acc: 60.10%\n",
      "[11] loss: 0.61701, acc: 61.42%\n",
      "[11] loss: 0.61119, acc: 61.54%\n",
      "[11] loss: 0.62896, acc: 61.78%\n",
      "[11] loss: 0.64640, acc: 60.34%\n",
      "[11] loss: 0.61498, acc: 64.18%\n",
      "[11] loss: 0.64442, acc: 57.81%\n",
      "[11] loss: 0.62437, acc: 60.70%\n",
      "[11] loss: 0.63008, acc: 62.26%\n",
      "[11] loss: 0.59499, acc: 63.46%\n",
      "[11] loss: 0.61967, acc: 62.50%\n",
      "[11] loss: 0.65220, acc: 58.89%\n",
      "[11] loss: 0.61020, acc: 62.38%\n",
      "[11] loss: 0.62436, acc: 60.34%\n",
      "[11] loss: 0.63122, acc: 62.02%\n",
      "[11] loss: 0.60693, acc: 62.86%\n",
      "[11] loss: 0.62617, acc: 62.98%\n",
      "[11] loss: 0.61352, acc: 62.14%\n",
      "[11] loss: 0.63009, acc: 62.50%\n",
      "[11] loss: 0.61868, acc: 60.10%\n",
      "[11] loss: 0.63367, acc: 57.93%\n",
      "[11] loss: 0.61329, acc: 62.02%\n",
      "[11] loss: 0.62364, acc: 61.30%\n",
      "[11] loss: 0.61099, acc: 61.78%\n",
      "[11] loss: 0.62159, acc: 61.18%\n",
      "[11] loss: 0.62108, acc: 62.38%\n",
      "[11] loss: 0.62430, acc: 61.18%\n",
      "[11] loss: 0.62819, acc: 61.18%\n",
      "[11] loss: 0.63027, acc: 60.70%\n",
      "[11] loss: 0.62228, acc: 62.50%\n",
      "[11] loss: 0.61974, acc: 62.62%\n",
      "[11] loss: 0.61964, acc: 61.42%\n",
      "[11] loss: 0.61741, acc: 62.74%\n",
      "[11] loss: 0.61482, acc: 62.86%\n",
      "[11] loss: 0.61002, acc: 63.58%\n",
      "[11] loss: 0.63442, acc: 59.98%\n",
      "[11] loss: 0.62876, acc: 60.58%\n",
      "[11] loss: 0.60170, acc: 63.58%\n",
      "[11] loss: 0.61592, acc: 62.86%\n",
      "[11] loss: 0.62848, acc: 61.90%\n",
      "[11] loss: 0.61919, acc: 62.62%\n",
      "[11] loss: 0.64793, acc: 61.78%\n",
      "[11] loss: 0.62236, acc: 63.46%\n",
      "[11] loss: 0.60663, acc: 61.90%\n",
      "[11] loss: 0.61890, acc: 62.50%\n",
      "[11] loss: 0.60459, acc: 63.10%\n",
      "[11] loss: 0.62544, acc: 60.58%\n",
      "[11] loss: 0.63445, acc: 58.89%\n",
      "[11] loss: 0.64310, acc: 60.46%\n",
      "[11] loss: 0.63594, acc: 60.70%\n",
      "[11] loss: 0.63740, acc: 58.53%\n",
      "[11] loss: 0.61632, acc: 61.54%\n",
      "[11] loss: 0.63626, acc: 59.50%\n",
      "[11] loss: 0.61971, acc: 63.34%\n",
      "[11] loss: 0.63842, acc: 60.58%\n",
      "[11] loss: 0.61913, acc: 61.42%\n",
      "[11] loss: 0.62457, acc: 61.30%\n",
      "[11] loss: 0.61919, acc: 59.86%\n",
      "[11] loss: 0.59691, acc: 62.26%\n",
      "[11] loss: 0.64394, acc: 58.29%\n",
      "[11] loss: 0.62646, acc: 61.66%\n",
      "[11] loss: 0.63552, acc: 60.34%\n",
      "[11] loss: 0.62491, acc: 59.25%\n",
      "[11] loss: 0.63162, acc: 60.70%\n",
      "[11] loss: 0.62497, acc: 61.30%\n",
      "[11] loss: 0.62156, acc: 62.26%\n",
      "[11] loss: 0.62874, acc: 62.86%\n",
      "[11] loss: 0.60793, acc: 64.78%\n",
      "[11] loss: 0.61207, acc: 62.26%\n",
      "[11] loss: 0.63269, acc: 60.58%\n",
      "[11] loss: 0.62714, acc: 59.25%\n",
      "[11] loss: 0.62708, acc: 61.06%\n",
      "[11] loss: 0.61249, acc: 62.14%\n",
      "[11] loss: 0.61694, acc: 60.70%\n",
      "[11] loss: 0.59759, acc: 64.54%\n",
      "[11] loss: 0.64713, acc: 59.13%\n",
      "[11] loss: 0.59110, acc: 64.66%\n",
      "[11] loss: 0.61405, acc: 62.74%\n",
      "[11] loss: 0.60699, acc: 60.34%\n",
      "[11] loss: 0.63822, acc: 59.98%\n",
      "[11] loss: 0.62702, acc: 62.14%\n",
      "[11] loss: 0.60068, acc: 62.14%\n",
      "[11] loss: 0.62116, acc: 60.58%\n",
      "[11] loss: 0.61673, acc: 62.14%\n",
      "[11] loss: 0.62081, acc: 60.58%\n",
      "[11] loss: 0.61721, acc: 62.98%\n",
      "[11] loss: 0.63153, acc: 62.38%\n",
      "[11] loss: 0.63397, acc: 62.86%\n",
      "[11] loss: 0.60924, acc: 62.98%\n",
      "[11] loss: 0.60825, acc: 60.70%\n",
      "[11] loss: 0.61253, acc: 63.34%\n",
      "[11] loss: 0.61398, acc: 61.90%\n",
      "[11] loss: 0.60975, acc: 60.34%\n",
      "[11] loss: 0.60460, acc: 63.82%\n",
      "[11] loss: 0.61408, acc: 62.62%\n",
      "[11] loss: 0.63460, acc: 59.62%\n",
      "[11] loss: 0.63774, acc: 59.74%\n",
      "[11] loss: 0.62989, acc: 61.78%\n",
      "[11] loss: 0.61394, acc: 61.42%\n",
      "[11] loss: 0.60924, acc: 61.54%\n",
      "[11] loss: 0.61955, acc: 62.14%\n",
      "[11] loss: 0.62634, acc: 61.30%\n",
      "[11] loss: 0.61554, acc: 62.62%\n",
      "[11] loss: 0.61962, acc: 61.90%\n",
      "[11] loss: 0.61734, acc: 62.02%\n",
      "[11] loss: 0.60026, acc: 63.82%\n",
      "[11] loss: 0.64827, acc: 60.94%\n",
      "[11] loss: 0.60760, acc: 61.42%\n",
      "[11] loss: 0.61959, acc: 62.26%\n",
      "[11] loss: 0.61864, acc: 63.10%\n",
      "[11] loss: 0.60817, acc: 62.62%\n",
      "[11] loss: 0.60375, acc: 64.66%\n",
      "[11] loss: 0.62360, acc: 62.26%\n",
      "[11] loss: 0.61370, acc: 63.82%\n",
      "[11] loss: 0.60030, acc: 62.26%\n",
      "[11] loss: 0.60402, acc: 63.70%\n",
      "[11] loss: 0.61757, acc: 60.94%\n",
      "[11] loss: 0.61543, acc: 62.98%\n",
      "[11] loss: 0.60207, acc: 62.14%\n",
      "[11] loss: 0.59444, acc: 64.18%\n",
      "[11] loss: 0.60228, acc: 64.18%\n",
      "[11] loss: 0.61589, acc: 62.50%\n",
      "[11] loss: 0.61020, acc: 63.94%\n",
      "[11] loss: 0.63980, acc: 62.74%\n",
      "[  11/1000] train_loss: 0.62066 valid_loss: 0.63475\n",
      "Validation loss decreased (0.644863 --> 0.634754).  Saving model ...\n",
      "========= Start Epoch 12 =========\n",
      "[12] loss: 0.58986, acc: 65.50%\n",
      "[12] loss: 0.60524, acc: 63.46%\n",
      "[12] loss: 0.59181, acc: 63.70%\n",
      "[12] loss: 0.61805, acc: 62.50%\n",
      "[12] loss: 0.64224, acc: 59.50%\n",
      "[12] loss: 0.63965, acc: 60.46%\n",
      "[12] loss: 0.58845, acc: 64.42%\n",
      "[12] loss: 0.60326, acc: 65.02%\n",
      "[12] loss: 0.61370, acc: 62.26%\n",
      "[12] loss: 0.62641, acc: 60.94%\n",
      "[12] loss: 0.62884, acc: 61.90%\n",
      "[12] loss: 0.63398, acc: 61.90%\n",
      "[12] loss: 0.60834, acc: 63.70%\n",
      "[12] loss: 0.62090, acc: 63.46%\n",
      "[12] loss: 0.61251, acc: 63.22%\n",
      "[12] loss: 0.60097, acc: 64.30%\n",
      "[12] loss: 0.61668, acc: 62.14%\n",
      "[12] loss: 0.60248, acc: 62.26%\n",
      "[12] loss: 0.60839, acc: 63.10%\n",
      "[12] loss: 0.61106, acc: 61.90%\n",
      "[12] loss: 0.64083, acc: 61.30%\n",
      "[12] loss: 0.60613, acc: 66.35%\n",
      "[12] loss: 0.63930, acc: 58.05%\n",
      "[12] loss: 0.61430, acc: 61.18%\n",
      "[12] loss: 0.61734, acc: 63.34%\n",
      "[12] loss: 0.57504, acc: 65.14%\n",
      "[12] loss: 0.61252, acc: 63.10%\n",
      "[12] loss: 0.63754, acc: 60.58%\n",
      "[12] loss: 0.59461, acc: 65.14%\n",
      "[12] loss: 0.61771, acc: 62.02%\n",
      "[12] loss: 0.62133, acc: 63.34%\n",
      "[12] loss: 0.58921, acc: 64.90%\n",
      "[12] loss: 0.61050, acc: 65.02%\n",
      "[12] loss: 0.59534, acc: 63.82%\n",
      "[12] loss: 0.62419, acc: 63.10%\n",
      "[12] loss: 0.60247, acc: 62.62%\n",
      "[12] loss: 0.62350, acc: 60.10%\n",
      "[12] loss: 0.60734, acc: 63.46%\n",
      "[12] loss: 0.61382, acc: 63.34%\n",
      "[12] loss: 0.60112, acc: 63.22%\n",
      "[12] loss: 0.60485, acc: 62.50%\n",
      "[12] loss: 0.60514, acc: 64.06%\n",
      "[12] loss: 0.61432, acc: 61.78%\n",
      "[12] loss: 0.61600, acc: 62.50%\n",
      "[12] loss: 0.61539, acc: 62.14%\n",
      "[12] loss: 0.60595, acc: 64.30%\n",
      "[12] loss: 0.61111, acc: 64.42%\n",
      "[12] loss: 0.60575, acc: 63.34%\n",
      "[12] loss: 0.60481, acc: 63.46%\n",
      "[12] loss: 0.61258, acc: 64.18%\n",
      "[12] loss: 0.59392, acc: 64.66%\n",
      "[12] loss: 0.62118, acc: 61.42%\n",
      "[12] loss: 0.62264, acc: 61.18%\n",
      "[12] loss: 0.58794, acc: 64.42%\n",
      "[12] loss: 0.60053, acc: 64.06%\n",
      "[12] loss: 0.61709, acc: 63.94%\n",
      "[12] loss: 0.61070, acc: 63.10%\n",
      "[12] loss: 0.62755, acc: 62.02%\n",
      "[12] loss: 0.60252, acc: 65.99%\n",
      "[12] loss: 0.58669, acc: 63.82%\n",
      "[12] loss: 0.60510, acc: 63.22%\n",
      "[12] loss: 0.58982, acc: 65.26%\n",
      "[12] loss: 0.59567, acc: 65.26%\n",
      "[12] loss: 0.62258, acc: 61.18%\n",
      "[12] loss: 0.62104, acc: 61.54%\n",
      "[12] loss: 0.63429, acc: 61.30%\n",
      "[12] loss: 0.62552, acc: 59.86%\n",
      "[12] loss: 0.60163, acc: 63.22%\n",
      "[12] loss: 0.62456, acc: 60.22%\n",
      "[12] loss: 0.61965, acc: 63.34%\n",
      "[12] loss: 0.61765, acc: 62.14%\n",
      "[12] loss: 0.61525, acc: 63.46%\n",
      "[12] loss: 0.61413, acc: 61.06%\n",
      "[12] loss: 0.60208, acc: 61.66%\n",
      "[12] loss: 0.57822, acc: 63.58%\n",
      "[12] loss: 0.62825, acc: 60.22%\n",
      "[12] loss: 0.61447, acc: 62.38%\n",
      "[12] loss: 0.62659, acc: 61.66%\n",
      "[12] loss: 0.60523, acc: 62.02%\n",
      "[12] loss: 0.61416, acc: 60.22%\n",
      "[12] loss: 0.60700, acc: 61.66%\n",
      "[12] loss: 0.59992, acc: 64.42%\n",
      "[12] loss: 0.61802, acc: 63.94%\n",
      "[12] loss: 0.60014, acc: 65.26%\n",
      "[12] loss: 0.60678, acc: 64.42%\n",
      "[12] loss: 0.62339, acc: 62.62%\n",
      "[12] loss: 0.61454, acc: 62.02%\n",
      "[12] loss: 0.61593, acc: 61.78%\n",
      "[12] loss: 0.59747, acc: 64.18%\n",
      "[12] loss: 0.59909, acc: 62.86%\n",
      "[12] loss: 0.58133, acc: 66.83%\n",
      "[12] loss: 0.63052, acc: 60.94%\n",
      "[12] loss: 0.57481, acc: 67.07%\n",
      "[12] loss: 0.59848, acc: 64.90%\n",
      "[12] loss: 0.60193, acc: 61.78%\n",
      "[12] loss: 0.62519, acc: 60.94%\n",
      "[12] loss: 0.62316, acc: 62.74%\n",
      "[12] loss: 0.58674, acc: 63.94%\n",
      "[12] loss: 0.61050, acc: 62.26%\n",
      "[12] loss: 0.61294, acc: 64.18%\n",
      "[12] loss: 0.60750, acc: 62.14%\n",
      "[12] loss: 0.60741, acc: 63.82%\n",
      "[12] loss: 0.62647, acc: 63.46%\n",
      "[12] loss: 0.61493, acc: 63.10%\n",
      "[12] loss: 0.59801, acc: 64.18%\n",
      "[12] loss: 0.60411, acc: 62.02%\n",
      "[12] loss: 0.59366, acc: 63.70%\n",
      "[12] loss: 0.59667, acc: 63.58%\n",
      "[12] loss: 0.60596, acc: 61.90%\n",
      "[12] loss: 0.58701, acc: 65.02%\n",
      "[12] loss: 0.59507, acc: 64.18%\n",
      "[12] loss: 0.61883, acc: 62.98%\n",
      "[12] loss: 0.62387, acc: 59.98%\n",
      "[12] loss: 0.61930, acc: 63.10%\n",
      "[12] loss: 0.60887, acc: 63.10%\n",
      "[12] loss: 0.59264, acc: 64.18%\n",
      "[12] loss: 0.61243, acc: 63.94%\n",
      "[12] loss: 0.61138, acc: 61.30%\n",
      "[12] loss: 0.61043, acc: 64.30%\n",
      "[12] loss: 0.60791, acc: 63.46%\n",
      "[12] loss: 0.59908, acc: 63.94%\n",
      "[12] loss: 0.58823, acc: 65.62%\n",
      "[12] loss: 0.62718, acc: 63.10%\n",
      "[12] loss: 0.58937, acc: 64.30%\n",
      "[12] loss: 0.61120, acc: 63.94%\n",
      "[12] loss: 0.59956, acc: 64.66%\n",
      "[12] loss: 0.59458, acc: 62.50%\n",
      "[12] loss: 0.59642, acc: 65.26%\n",
      "[12] loss: 0.60320, acc: 62.50%\n",
      "[12] loss: 0.60204, acc: 65.62%\n",
      "[12] loss: 0.58572, acc: 64.42%\n",
      "[12] loss: 0.59433, acc: 63.94%\n",
      "[12] loss: 0.60430, acc: 62.50%\n",
      "[12] loss: 0.59718, acc: 65.26%\n",
      "[12] loss: 0.59489, acc: 63.58%\n",
      "[12] loss: 0.57911, acc: 64.90%\n",
      "[12] loss: 0.58297, acc: 64.42%\n",
      "[12] loss: 0.59514, acc: 62.98%\n",
      "[12] loss: 0.59400, acc: 64.42%\n",
      "[12] loss: 0.62528, acc: 64.06%\n",
      "[  12/1000] train_loss: 0.60766 valid_loss: 0.63390\n",
      "Validation loss decreased (0.634754 --> 0.633897).  Saving model ...\n",
      "========= Start Epoch 13 =========\n",
      "[13] loss: 0.57367, acc: 65.75%\n",
      "[13] loss: 0.60108, acc: 64.54%\n",
      "[13] loss: 0.58824, acc: 65.87%\n",
      "[13] loss: 0.61032, acc: 63.70%\n",
      "[13] loss: 0.63267, acc: 60.34%\n",
      "[13] loss: 0.62239, acc: 62.74%\n",
      "[13] loss: 0.56344, acc: 65.75%\n",
      "[13] loss: 0.59343, acc: 65.02%\n",
      "[13] loss: 0.59297, acc: 63.46%\n",
      "[13] loss: 0.60960, acc: 62.74%\n",
      "[13] loss: 0.62176, acc: 62.62%\n",
      "[13] loss: 0.62759, acc: 61.54%\n",
      "[13] loss: 0.59549, acc: 64.54%\n",
      "[13] loss: 0.60254, acc: 64.66%\n",
      "[13] loss: 0.59818, acc: 64.18%\n",
      "[13] loss: 0.58967, acc: 65.62%\n",
      "[13] loss: 0.60227, acc: 62.62%\n",
      "[13] loss: 0.59298, acc: 64.06%\n",
      "[13] loss: 0.59778, acc: 63.94%\n",
      "[13] loss: 0.59531, acc: 64.42%\n",
      "[13] loss: 0.62358, acc: 63.22%\n",
      "[13] loss: 0.60110, acc: 67.07%\n",
      "[13] loss: 0.62361, acc: 60.34%\n",
      "[13] loss: 0.60153, acc: 63.70%\n",
      "[13] loss: 0.59586, acc: 64.06%\n",
      "[13] loss: 0.55453, acc: 67.67%\n",
      "[13] loss: 0.60526, acc: 64.90%\n",
      "[13] loss: 0.62614, acc: 61.66%\n",
      "[13] loss: 0.57720, acc: 65.99%\n",
      "[13] loss: 0.60076, acc: 63.70%\n",
      "[13] loss: 0.60745, acc: 64.18%\n",
      "[13] loss: 0.58673, acc: 64.66%\n",
      "[13] loss: 0.59994, acc: 66.11%\n",
      "[13] loss: 0.59092, acc: 64.18%\n",
      "[13] loss: 0.61459, acc: 62.50%\n",
      "[13] loss: 0.58808, acc: 63.82%\n",
      "[13] loss: 0.61342, acc: 61.90%\n",
      "[13] loss: 0.59850, acc: 64.42%\n",
      "[13] loss: 0.59676, acc: 64.18%\n",
      "[13] loss: 0.58311, acc: 63.70%\n",
      "[13] loss: 0.58740, acc: 64.42%\n",
      "[13] loss: 0.59154, acc: 64.42%\n",
      "[13] loss: 0.59924, acc: 62.74%\n",
      "[13] loss: 0.60790, acc: 64.42%\n",
      "[13] loss: 0.59718, acc: 64.90%\n",
      "[13] loss: 0.58508, acc: 66.35%\n",
      "[13] loss: 0.60207, acc: 65.02%\n",
      "[13] loss: 0.58985, acc: 64.54%\n",
      "[13] loss: 0.58497, acc: 65.87%\n",
      "[13] loss: 0.59835, acc: 65.87%\n",
      "[13] loss: 0.58461, acc: 66.11%\n",
      "[13] loss: 0.61076, acc: 62.98%\n",
      "[13] loss: 0.61028, acc: 62.86%\n",
      "[13] loss: 0.57347, acc: 64.90%\n",
      "[13] loss: 0.58416, acc: 64.90%\n",
      "[13] loss: 0.61169, acc: 63.22%\n",
      "[13] loss: 0.58223, acc: 65.26%\n",
      "[13] loss: 0.62406, acc: 63.82%\n",
      "[13] loss: 0.59879, acc: 65.26%\n",
      "[13] loss: 0.57520, acc: 64.30%\n",
      "[13] loss: 0.58686, acc: 65.62%\n",
      "[13] loss: 0.58080, acc: 66.95%\n",
      "[13] loss: 0.57879, acc: 65.99%\n",
      "[13] loss: 0.60876, acc: 62.50%\n",
      "[13] loss: 0.60503, acc: 63.94%\n",
      "[13] loss: 0.61840, acc: 62.98%\n",
      "[13] loss: 0.61124, acc: 62.62%\n",
      "[13] loss: 0.59104, acc: 64.18%\n",
      "[13] loss: 0.61546, acc: 62.86%\n",
      "[13] loss: 0.60565, acc: 64.18%\n",
      "[13] loss: 0.61098, acc: 64.54%\n",
      "[13] loss: 0.59843, acc: 63.70%\n",
      "[13] loss: 0.61012, acc: 63.46%\n",
      "[13] loss: 0.58625, acc: 63.58%\n",
      "[13] loss: 0.55648, acc: 67.43%\n",
      "[13] loss: 0.61121, acc: 63.10%\n",
      "[13] loss: 0.59949, acc: 62.98%\n",
      "[13] loss: 0.61762, acc: 61.78%\n",
      "[13] loss: 0.59768, acc: 64.18%\n",
      "[13] loss: 0.60574, acc: 62.74%\n",
      "[13] loss: 0.58555, acc: 65.26%\n",
      "[13] loss: 0.58041, acc: 65.26%\n",
      "[13] loss: 0.60152, acc: 64.78%\n",
      "[13] loss: 0.58212, acc: 66.35%\n",
      "[13] loss: 0.58738, acc: 65.99%\n",
      "[13] loss: 0.60951, acc: 63.10%\n",
      "[13] loss: 0.59506, acc: 63.70%\n",
      "[13] loss: 0.61526, acc: 62.86%\n",
      "[13] loss: 0.58486, acc: 66.11%\n",
      "[13] loss: 0.59306, acc: 64.42%\n",
      "[13] loss: 0.56623, acc: 68.15%\n",
      "[13] loss: 0.61409, acc: 62.38%\n",
      "[13] loss: 0.56113, acc: 68.51%\n",
      "[13] loss: 0.57621, acc: 65.62%\n",
      "[13] loss: 0.58947, acc: 63.82%\n",
      "[13] loss: 0.62075, acc: 63.22%\n",
      "[13] loss: 0.61522, acc: 62.86%\n",
      "[13] loss: 0.57514, acc: 64.18%\n",
      "[13] loss: 0.59625, acc: 62.86%\n",
      "[13] loss: 0.59406, acc: 65.75%\n",
      "[13] loss: 0.59688, acc: 63.10%\n",
      "[13] loss: 0.59661, acc: 65.38%\n",
      "[13] loss: 0.61271, acc: 65.50%\n",
      "[13] loss: 0.59832, acc: 65.62%\n",
      "[13] loss: 0.57457, acc: 66.47%\n",
      "[13] loss: 0.58848, acc: 64.42%\n",
      "[13] loss: 0.58170, acc: 66.83%\n",
      "[13] loss: 0.58264, acc: 63.82%\n",
      "[13] loss: 0.59348, acc: 62.86%\n",
      "[13] loss: 0.56872, acc: 66.35%\n",
      "[13] loss: 0.57853, acc: 65.50%\n",
      "[13] loss: 0.59542, acc: 65.14%\n",
      "[13] loss: 0.60529, acc: 63.10%\n",
      "[13] loss: 0.61089, acc: 62.38%\n",
      "[13] loss: 0.58483, acc: 65.02%\n",
      "[13] loss: 0.57997, acc: 65.38%\n",
      "[13] loss: 0.59913, acc: 65.14%\n",
      "[13] loss: 0.59764, acc: 63.70%\n",
      "[13] loss: 0.58968, acc: 64.90%\n",
      "[13] loss: 0.59162, acc: 65.62%\n",
      "[13] loss: 0.59047, acc: 65.26%\n",
      "[13] loss: 0.57410, acc: 66.11%\n",
      "[13] loss: 0.60778, acc: 64.78%\n",
      "[13] loss: 0.58395, acc: 64.18%\n",
      "[13] loss: 0.60995, acc: 64.06%\n",
      "[13] loss: 0.58417, acc: 66.35%\n",
      "[13] loss: 0.58676, acc: 63.46%\n",
      "[13] loss: 0.57568, acc: 67.19%\n",
      "[13] loss: 0.59058, acc: 63.34%\n",
      "[13] loss: 0.58307, acc: 67.19%\n",
      "[13] loss: 0.56918, acc: 66.95%\n",
      "[13] loss: 0.57534, acc: 65.99%\n",
      "[13] loss: 0.59014, acc: 64.18%\n",
      "[13] loss: 0.57317, acc: 67.55%\n",
      "[13] loss: 0.57425, acc: 65.87%\n",
      "[13] loss: 0.55619, acc: 67.43%\n",
      "[13] loss: 0.56100, acc: 68.03%\n",
      "[13] loss: 0.58092, acc: 65.26%\n",
      "[13] loss: 0.58754, acc: 64.90%\n",
      "[13] loss: 0.59830, acc: 66.35%\n",
      "[  13/1000] train_loss: 0.59438 valid_loss: 0.62391\n",
      "Validation loss decreased (0.633897 --> 0.623909).  Saving model ...\n",
      "========= Start Epoch 14 =========\n",
      "[14] loss: 0.54467, acc: 68.15%\n",
      "[14] loss: 0.59240, acc: 65.50%\n",
      "[14] loss: 0.56426, acc: 67.79%\n",
      "[14] loss: 0.59636, acc: 63.82%\n",
      "[14] loss: 0.62182, acc: 62.26%\n",
      "[14] loss: 0.60774, acc: 64.90%\n",
      "[14] loss: 0.54432, acc: 68.39%\n",
      "[14] loss: 0.58090, acc: 66.11%\n",
      "[14] loss: 0.58120, acc: 63.94%\n",
      "[14] loss: 0.60230, acc: 64.90%\n",
      "[14] loss: 0.59816, acc: 64.78%\n",
      "[14] loss: 0.60898, acc: 64.30%\n",
      "[14] loss: 0.57795, acc: 67.43%\n",
      "[14] loss: 0.57822, acc: 65.87%\n",
      "[14] loss: 0.59005, acc: 63.94%\n",
      "[14] loss: 0.57619, acc: 67.67%\n",
      "[14] loss: 0.58594, acc: 65.02%\n",
      "[14] loss: 0.56842, acc: 66.59%\n",
      "[14] loss: 0.58265, acc: 65.50%\n",
      "[14] loss: 0.58028, acc: 65.14%\n",
      "[14] loss: 0.60875, acc: 65.14%\n",
      "[14] loss: 0.59031, acc: 67.19%\n",
      "[14] loss: 0.60087, acc: 62.02%\n",
      "[14] loss: 0.59022, acc: 65.26%\n",
      "[14] loss: 0.58599, acc: 66.11%\n",
      "[14] loss: 0.53726, acc: 69.47%\n",
      "[14] loss: 0.59366, acc: 65.99%\n",
      "[14] loss: 0.60136, acc: 64.30%\n",
      "[14] loss: 0.56802, acc: 67.31%\n",
      "[14] loss: 0.59185, acc: 65.14%\n",
      "[14] loss: 0.58168, acc: 66.59%\n",
      "[14] loss: 0.58454, acc: 65.38%\n",
      "[14] loss: 0.58993, acc: 65.99%\n",
      "[14] loss: 0.57329, acc: 65.99%\n",
      "[14] loss: 0.59996, acc: 64.06%\n",
      "[14] loss: 0.56065, acc: 66.47%\n",
      "[14] loss: 0.59667, acc: 65.02%\n",
      "[14] loss: 0.58206, acc: 65.87%\n",
      "[14] loss: 0.59185, acc: 64.18%\n",
      "[14] loss: 0.57228, acc: 65.38%\n",
      "[14] loss: 0.57820, acc: 65.87%\n",
      "[14] loss: 0.57994, acc: 65.14%\n",
      "[14] loss: 0.58295, acc: 64.30%\n",
      "[14] loss: 0.58985, acc: 65.87%\n",
      "[14] loss: 0.58187, acc: 65.87%\n",
      "[14] loss: 0.56465, acc: 66.71%\n",
      "[14] loss: 0.57851, acc: 67.79%\n",
      "[14] loss: 0.57186, acc: 65.50%\n",
      "[14] loss: 0.56176, acc: 68.27%\n",
      "[14] loss: 0.58641, acc: 68.15%\n",
      "[14] loss: 0.55919, acc: 69.59%\n",
      "[14] loss: 0.60663, acc: 64.54%\n",
      "[14] loss: 0.59168, acc: 65.87%\n",
      "[14] loss: 0.55355, acc: 67.07%\n",
      "[14] loss: 0.56599, acc: 65.75%\n",
      "[14] loss: 0.59044, acc: 65.26%\n",
      "[14] loss: 0.56742, acc: 66.35%\n",
      "[14] loss: 0.60999, acc: 64.90%\n",
      "[14] loss: 0.59080, acc: 66.47%\n",
      "[14] loss: 0.55653, acc: 65.99%\n",
      "[14] loss: 0.57534, acc: 67.07%\n",
      "[14] loss: 0.56210, acc: 68.15%\n",
      "[14] loss: 0.57007, acc: 68.27%\n",
      "[14] loss: 0.58970, acc: 63.94%\n",
      "[14] loss: 0.58694, acc: 66.59%\n",
      "[14] loss: 0.59944, acc: 64.66%\n",
      "[14] loss: 0.59489, acc: 65.38%\n",
      "[14] loss: 0.56978, acc: 66.47%\n",
      "[14] loss: 0.58935, acc: 64.42%\n",
      "[14] loss: 0.58846, acc: 65.26%\n",
      "[14] loss: 0.60224, acc: 64.78%\n",
      "[14] loss: 0.59072, acc: 64.42%\n",
      "[14] loss: 0.59243, acc: 63.94%\n",
      "[14] loss: 0.57608, acc: 64.42%\n",
      "[14] loss: 0.54277, acc: 69.23%\n",
      "[14] loss: 0.60404, acc: 64.30%\n",
      "[14] loss: 0.58773, acc: 64.42%\n",
      "[14] loss: 0.60801, acc: 64.54%\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7e89060286ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-2333b6756d41>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 3, verbose = True)\n",
    "\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    model.train()\n",
    "    print('========= Start Epoch {} ========='.format(epoch))\n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            outputs = outputs > 0.5\n",
    "            acc = (outputs == targets).float().mean()\n",
    "            print(f'[{epoch}] loss: {loss.item():.5f}, acc: {acc.item() * 100:.2f}%')\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for i, (images, targets) in enumerate(valid_loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    \n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "\n",
    "    epoch_len = len(str(num_epochs))\n",
    "\n",
    "    print_msg = (f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "\n",
    "    print(print_msg)\n",
    "\n",
    "    # clear lists to track next epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    early_stopping(valid_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "source": [
    "## 5. 추론하기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('2nd_data/sample_submission.csv')\n",
    "\n",
    "model.eval()\n",
    "batch_size = test_loader.batch_size\n",
    "batch_index = 0\n",
    "for i, (images, targets) in enumerate(test_loader):\n",
    "    images = images.to(device)\n",
    "    targets = targets.to(device)\n",
    "    outputs = model(images)\n",
    "    outputs = outputs > 0.5\n",
    "    batch_index = i * batch_size\n",
    "    submit.iloc[batch_index:batch_index+batch_size, 1:] = \\\n",
    "        outputs.long().squeeze(0).detach().cpu().numpy()\n",
    "    \n",
    "submit.to_csv('submit_augmix.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}