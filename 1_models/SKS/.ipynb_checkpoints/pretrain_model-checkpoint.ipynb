{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from src.model import DaconModel\n",
    "\n",
    "from utils.imageprocess import image_transformer, image_processor\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from utils.dataloader import CustomDataLoader\n",
    "from utils.radams import RAdam\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopper\n",
    "from utils.EarlyStopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_dict = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((40, 40)),\n",
    "        transforms.RandomPerspective(),\n",
    "        transforms.Pad(108, fill=0, padding_mode='constant'),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((40, 40)),\n",
    "        transforms.Pad(108, fill=0, padding_mode='constant'),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "}\n",
    "\n",
    "trainset = torchvision.datasets.EMNIST(root='./dataset/emnist_dataset',\n",
    "                                       split='letters',\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=transform_dict['train'])\n",
    "\n",
    "testset = torchvision.datasets.EMNIST(root='./dataset/emnist_dataset',\n",
    "                                      split='letters',\n",
    "                                      train=False,\n",
    "                                      download=True,\n",
    "                                      transform=transform_dict['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = 0.8\n",
    "# train_len = int(len(trainset) * train_ratio)\n",
    "# val_len = len(trainset) - train_len\n",
    "# logger.info(f'Train Length: {train_len}, Validation Length: {val_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, val_set = torch.utils.data.random_split(trainset, lengths=[train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "    elif classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "def train_model(model=None, epochs=50, learning_rate=0.0027, *loaders):\n",
    "    \n",
    "    #model.apply(weights_init)\n",
    "    \n",
    "    # Dataloaders (train/val)\n",
    "    train_loader = loaders[0]\n",
    "    val_loader = loaders[1]\n",
    "    \n",
    "    # Early Stopper\n",
    "    early_stopping = EarlyStopping(patience=10, verbose=False, path='./pretrained_resnet.pth')\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = RAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "\n",
    "    decayRate = 0.998\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "    \n",
    "    train_tot_num = train_loader.dataset.__len__()\n",
    "    val_tot_num = val_loader.dataset.__len__()\n",
    "    \n",
    "    train_loss_sum, train_acc_sum = 0.0, 0.0\n",
    "    val_loss_sum, val_acc_sum = 0.0, 0.0\n",
    "    \n",
    "    logger.info(f'Training begins: epochs={epochs}')\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        \n",
    "        train_tmp_corrects_sum, train_tmp_len = 0, 0\n",
    "        for idx, (train_X, train_Y) in enumerate(train_loader):    \n",
    "            \n",
    "            train_tmp_len += len(train_Y)\n",
    "            \n",
    "            train_X = train_X.to(device)\n",
    "            train_Y = train_Y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_pred = model(train_X)\n",
    "            train_loss = loss_function(train_pred, train_Y-1)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accuracy\n",
    "            pred_labels = torch.argmax(train_pred, axis=1)\n",
    "            train_acc = (pred_labels==(train_Y-1)).sum()\n",
    "            \n",
    "            train_acc_sum += train_acc.item()\n",
    "            train_loss_sum += train_loss.item()\n",
    "            \n",
    "            train_tmp_corrects_sum += train_acc.item()\n",
    "            \n",
    "            # Check between batches\n",
    "            if (idx+1) % 200 == 0:\n",
    "                print(f\"----- {str((idx+1)).zfill(4)}th batch | Train Acc: {train_tmp_corrects_sum/train_tmp_len*100:.4f}%\")\n",
    "                \n",
    "                # initialization\n",
    "                train_tmp_corrects_sum, train_tmp_len = 0, 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for idx, (val_X, val_Y) in enumerate(val_loader):\n",
    "                \n",
    "                val_X, val_Y = val_X.to(device), val_Y.to(device)\n",
    "                val_pred = model(val_X)\n",
    "                val_loss = loss_function(val_pred, val_Y-1)\n",
    "                \n",
    "                val_pred_labels = torch.argmax(val_pred, axis=1)\n",
    "                val_acc = (val_pred_labels==(val_Y-1)).sum()\n",
    "                \n",
    "                val_acc_sum += val_acc.item()\n",
    "                val_loss_sum += val_loss.item()\n",
    "        \n",
    "        train_accuracy = train_acc_sum / train_tot_num * 100\n",
    "        val_accuracy = val_acc_sum / val_tot_num * 100\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1} | Training Loss: {train_loss_sum/len(train_loader):.4f} | Val Loss: {val_loss_sum/len(val_loader):.4f} | \" \\\n",
    "              f\"Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        train_loss_sum, train_acc_sum = 0.0, 0.0\n",
    "        val_loss_sum, val_acc_sum = 0.0, 0.0\n",
    "        \n",
    "        early_stopping(-val_accuracy, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping condition met --- TRAINING STOPPED\")\n",
    "            break\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0027\n",
    "epochs = 100\n",
    "\n",
    "resnet_model = resnet50().to(device)\n",
    "resnet_model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    \n",
    "    nn.Linear(256, 26),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PretrainedModel, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            resnet_model,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "# 모델 선언\n",
    "model = PretrainedModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2048, out_features=256, bias=True)\n",
       "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): Linear(in_features=256, out_features=26, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.block[0].fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-06 15:14:47,747 : Training begins: epochs=100\n",
      "/home/sks/anaconda3/envs/dacon/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\n",
      "  warnings.warn(\"Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\")\n",
      "/home/sks/COMPETITION/DACON/computer_vision2/utils/radams.py:45: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 0200th batch | Train Acc: 46.8203%\n",
      "----- 0400th batch | Train Acc: 75.4219%\n",
      "----- 0600th batch | Train Acc: 81.4453%\n",
      "----- 0800th batch | Train Acc: 83.2109%\n",
      "----- 1000th batch | Train Acc: 85.6953%\n",
      "----- 1200th batch | Train Acc: 85.7812%\n",
      "----- 1400th batch | Train Acc: 86.0078%\n",
      "----- 1600th batch | Train Acc: 87.3125%\n",
      "----- 1800th batch | Train Acc: 87.6328%\n",
      "Epoch: 1 | Training Loss: 0.6079 | Val Loss: 0.2697 | Train Acc: 80.54% | Val Acc: 91.50%\n",
      "----- 0200th batch | Train Acc: 88.6406%\n",
      "----- 0400th batch | Train Acc: 89.2344%\n",
      "----- 0600th batch | Train Acc: 88.9922%\n",
      "----- 0800th batch | Train Acc: 89.0703%\n",
      "----- 1000th batch | Train Acc: 89.8438%\n",
      "----- 1200th batch | Train Acc: 89.8906%\n",
      "----- 1400th batch | Train Acc: 89.7578%\n",
      "----- 1600th batch | Train Acc: 89.9531%\n",
      "----- 1800th batch | Train Acc: 90.6250%\n",
      "Epoch: 2 | Training Loss: 0.3130 | Val Loss: 0.2716 | Train Acc: 89.62% | Val Acc: 90.48%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "----- 0200th batch | Train Acc: 90.5547%\n",
      "----- 0400th batch | Train Acc: 90.6641%\n",
      "----- 0600th batch | Train Acc: 90.8672%\n",
      "----- 0800th batch | Train Acc: 91.1172%\n",
      "----- 1000th batch | Train Acc: 90.6016%\n",
      "----- 1200th batch | Train Acc: 90.8516%\n",
      "----- 1400th batch | Train Acc: 90.9688%\n",
      "----- 1600th batch | Train Acc: 90.9688%\n",
      "----- 1800th batch | Train Acc: 91.5703%\n",
      "Epoch: 3 | Training Loss: 0.2713 | Val Loss: 0.2251 | Train Acc: 90.98% | Val Acc: 92.33%\n",
      "----- 0200th batch | Train Acc: 91.3281%\n",
      "----- 0400th batch | Train Acc: 91.1953%\n",
      "----- 0600th batch | Train Acc: 91.7031%\n",
      "----- 0800th batch | Train Acc: 92.2578%\n",
      "----- 1000th batch | Train Acc: 91.1641%\n",
      "----- 1200th batch | Train Acc: 91.8359%\n",
      "----- 1400th batch | Train Acc: 92.1172%\n",
      "----- 1600th batch | Train Acc: 91.7422%\n",
      "----- 1800th batch | Train Acc: 91.7188%\n",
      "Epoch: 4 | Training Loss: 0.2472 | Val Loss: 0.1850 | Train Acc: 91.68% | Val Acc: 93.93%\n",
      "----- 0200th batch | Train Acc: 92.4531%\n",
      "----- 0400th batch | Train Acc: 92.3359%\n",
      "----- 0600th batch | Train Acc: 92.3438%\n",
      "----- 0800th batch | Train Acc: 92.1641%\n",
      "----- 1000th batch | Train Acc: 92.0781%\n",
      "----- 1200th batch | Train Acc: 92.4141%\n",
      "----- 1400th batch | Train Acc: 92.3438%\n",
      "----- 1600th batch | Train Acc: 92.4531%\n",
      "----- 1800th batch | Train Acc: 92.7031%\n",
      "Epoch: 5 | Training Loss: 0.2250 | Val Loss: 0.2065 | Train Acc: 92.37% | Val Acc: 92.62%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "----- 0200th batch | Train Acc: 92.6562%\n",
      "----- 0400th batch | Train Acc: 92.7656%\n",
      "----- 0600th batch | Train Acc: 92.6562%\n",
      "----- 0800th batch | Train Acc: 92.9844%\n",
      "----- 1000th batch | Train Acc: 92.7656%\n",
      "----- 1200th batch | Train Acc: 92.7188%\n",
      "----- 1400th batch | Train Acc: 93.5625%\n",
      "----- 1600th batch | Train Acc: 92.8828%\n",
      "----- 1800th batch | Train Acc: 93.1328%\n",
      "Epoch: 6 | Training Loss: 0.2063 | Val Loss: 0.1817 | Train Acc: 92.89% | Val Acc: 93.71%\n",
      "EarlyStopping counter: 2 out of 10\n",
      "----- 0200th batch | Train Acc: 93.1406%\n",
      "----- 0400th batch | Train Acc: 93.2969%\n",
      "----- 0600th batch | Train Acc: 93.6484%\n",
      "----- 0800th batch | Train Acc: 93.1328%\n",
      "----- 1000th batch | Train Acc: 93.2031%\n",
      "----- 1200th batch | Train Acc: 93.1406%\n",
      "----- 1400th batch | Train Acc: 93.3203%\n",
      "----- 1600th batch | Train Acc: 93.1406%\n",
      "----- 1800th batch | Train Acc: 93.0547%\n",
      "Epoch: 7 | Training Loss: 0.1941 | Val Loss: 0.1666 | Train Acc: 93.26% | Val Acc: 94.40%\n",
      "----- 0200th batch | Train Acc: 93.3438%\n",
      "----- 0400th batch | Train Acc: 93.8516%\n",
      "----- 0600th batch | Train Acc: 93.5938%\n",
      "----- 0800th batch | Train Acc: 93.4844%\n",
      "----- 1000th batch | Train Acc: 93.6875%\n",
      "----- 1200th batch | Train Acc: 93.8984%\n",
      "----- 1400th batch | Train Acc: 93.3906%\n",
      "----- 1600th batch | Train Acc: 93.6484%\n",
      "----- 1800th batch | Train Acc: 93.6250%\n",
      "Epoch: 8 | Training Loss: 0.1827 | Val Loss: 0.1673 | Train Acc: 93.61% | Val Acc: 94.24%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "----- 0200th batch | Train Acc: 94.1328%\n",
      "----- 0400th batch | Train Acc: 94.0312%\n",
      "----- 0600th batch | Train Acc: 94.3516%\n",
      "----- 0800th batch | Train Acc: 93.4844%\n",
      "----- 1000th batch | Train Acc: 94.0078%\n",
      "----- 1200th batch | Train Acc: 93.8438%\n",
      "----- 1400th batch | Train Acc: 93.9375%\n",
      "----- 1600th batch | Train Acc: 94.2891%\n",
      "----- 1800th batch | Train Acc: 93.9844%\n",
      "Epoch: 9 | Training Loss: 0.1726 | Val Loss: 0.1593 | Train Acc: 94.01% | Val Acc: 94.71%\n",
      "----- 0200th batch | Train Acc: 93.9531%\n",
      "----- 0400th batch | Train Acc: 94.0625%\n",
      "----- 0600th batch | Train Acc: 94.0391%\n",
      "----- 0800th batch | Train Acc: 94.0781%\n",
      "----- 1000th batch | Train Acc: 94.2031%\n",
      "----- 1200th batch | Train Acc: 94.4141%\n",
      "----- 1400th batch | Train Acc: 93.6953%\n",
      "----- 1600th batch | Train Acc: 94.1953%\n",
      "----- 1800th batch | Train Acc: 93.6484%\n",
      "Epoch: 10 | Training Loss: 0.1666 | Val Loss: 0.1582 | Train Acc: 94.06% | Val Acc: 94.75%\n",
      "----- 0200th batch | Train Acc: 94.4609%\n",
      "----- 0400th batch | Train Acc: 94.2344%\n",
      "----- 0600th batch | Train Acc: 94.4141%\n",
      "----- 0800th batch | Train Acc: 94.0625%\n",
      "----- 1000th batch | Train Acc: 94.4531%\n",
      "----- 1200th batch | Train Acc: 94.4844%\n",
      "----- 1400th batch | Train Acc: 94.5391%\n",
      "----- 1600th batch | Train Acc: 94.2891%\n",
      "----- 1800th batch | Train Acc: 94.8359%\n",
      "Epoch: 11 | Training Loss: 0.1585 | Val Loss: 0.1546 | Train Acc: 94.40% | Val Acc: 94.74%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "----- 0200th batch | Train Acc: 94.7422%\n",
      "----- 0400th batch | Train Acc: 94.6875%\n",
      "----- 0600th batch | Train Acc: 93.8906%\n",
      "----- 0800th batch | Train Acc: 94.2188%\n",
      "----- 1000th batch | Train Acc: 94.7500%\n",
      "----- 1200th batch | Train Acc: 94.4219%\n",
      "----- 1400th batch | Train Acc: 94.3203%\n",
      "----- 1600th batch | Train Acc: 94.6328%\n",
      "----- 1800th batch | Train Acc: 94.4219%\n",
      "Epoch: 12 | Training Loss: 0.1543 | Val Loss: 0.1552 | Train Acc: 94.44% | Val Acc: 94.70%\n",
      "EarlyStopping counter: 2 out of 10\n",
      "----- 0200th batch | Train Acc: 94.7188%\n",
      "----- 0400th batch | Train Acc: 94.4922%\n",
      "----- 0600th batch | Train Acc: 94.8281%\n",
      "----- 0800th batch | Train Acc: 94.6641%\n",
      "----- 1000th batch | Train Acc: 94.2656%\n",
      "----- 1200th batch | Train Acc: 94.4219%\n",
      "----- 1400th batch | Train Acc: 94.6406%\n",
      "----- 1600th batch | Train Acc: 94.8984%\n",
      "----- 1800th batch | Train Acc: 94.5859%\n",
      "Epoch: 13 | Training Loss: 0.1496 | Val Loss: 0.1450 | Train Acc: 94.64% | Val Acc: 94.92%\n",
      "----- 0200th batch | Train Acc: 95.0156%\n",
      "----- 0400th batch | Train Acc: 94.9141%\n",
      "----- 0600th batch | Train Acc: 94.9453%\n",
      "----- 0800th batch | Train Acc: 94.7266%\n",
      "----- 1000th batch | Train Acc: 94.8281%\n",
      "----- 1200th batch | Train Acc: 94.7266%\n",
      "----- 1400th batch | Train Acc: 94.3750%\n",
      "----- 1600th batch | Train Acc: 94.8984%\n",
      "----- 1800th batch | Train Acc: 94.7188%\n",
      "Epoch: 14 | Training Loss: 0.1446 | Val Loss: 0.1552 | Train Acc: 94.78% | Val Acc: 94.76%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "----- 0200th batch | Train Acc: 94.7500%\n",
      "----- 0400th batch | Train Acc: 94.5938%\n",
      "----- 0600th batch | Train Acc: 94.9062%\n",
      "----- 0800th batch | Train Acc: 94.9375%\n",
      "----- 1000th batch | Train Acc: 95.0781%\n",
      "----- 1200th batch | Train Acc: 94.9375%\n",
      "----- 1400th batch | Train Acc: 95.1797%\n",
      "----- 1600th batch | Train Acc: 94.7891%\n",
      "----- 1800th batch | Train Acc: 94.9375%\n",
      "Epoch: 15 | Training Loss: 0.1393 | Val Loss: 0.1555 | Train Acc: 94.91% | Val Acc: 94.53%\n",
      "EarlyStopping counter: 2 out of 10\n",
      "----- 0200th batch | Train Acc: 94.8125%\n",
      "----- 0400th batch | Train Acc: 94.9141%\n",
      "----- 0600th batch | Train Acc: 94.8672%\n",
      "----- 0800th batch | Train Acc: 94.9531%\n",
      "----- 1000th batch | Train Acc: 95.0938%\n",
      "----- 1200th batch | Train Acc: 95.1094%\n",
      "----- 1400th batch | Train Acc: 94.9688%\n",
      "----- 1600th batch | Train Acc: 94.8984%\n",
      "----- 1800th batch | Train Acc: 94.7969%\n",
      "Epoch: 16 | Training Loss: 0.1372 | Val Loss: 0.1507 | Train Acc: 94.94% | Val Acc: 94.74%\n",
      "EarlyStopping counter: 3 out of 10\n",
      "----- 0200th batch | Train Acc: 95.0703%\n",
      "----- 0400th batch | Train Acc: 95.1875%\n",
      "----- 0600th batch | Train Acc: 95.2500%\n",
      "----- 0800th batch | Train Acc: 95.1328%\n",
      "----- 1000th batch | Train Acc: 95.0938%\n",
      "----- 1200th batch | Train Acc: 94.9375%\n",
      "----- 1400th batch | Train Acc: 95.0625%\n",
      "----- 1600th batch | Train Acc: 95.0938%\n",
      "----- 1800th batch | Train Acc: 95.0547%\n",
      "Epoch: 17 | Training Loss: 0.1342 | Val Loss: 0.1519 | Train Acc: 95.09% | Val Acc: 94.74%\n",
      "EarlyStopping counter: 4 out of 10\n",
      "----- 0200th batch | Train Acc: 95.4766%\n",
      "----- 0400th batch | Train Acc: 95.1250%\n",
      "----- 0600th batch | Train Acc: 95.0391%\n",
      "----- 0800th batch | Train Acc: 95.0781%\n",
      "----- 1000th batch | Train Acc: 94.9375%\n",
      "----- 1200th batch | Train Acc: 95.0781%\n",
      "----- 1400th batch | Train Acc: 95.5234%\n",
      "----- 1600th batch | Train Acc: 95.1328%\n",
      "----- 1800th batch | Train Acc: 95.2500%\n",
      "Epoch: 18 | Training Loss: 0.1297 | Val Loss: 0.1485 | Train Acc: 95.19% | Val Acc: 94.89%\n",
      "EarlyStopping counter: 5 out of 10\n",
      "----- 0200th batch | Train Acc: 95.1875%\n",
      "----- 0400th batch | Train Acc: 95.2188%\n",
      "----- 0600th batch | Train Acc: 95.1250%\n",
      "----- 0800th batch | Train Acc: 95.3438%\n",
      "----- 1000th batch | Train Acc: 95.4219%\n",
      "----- 1200th batch | Train Acc: 95.3281%\n",
      "----- 1400th batch | Train Acc: 94.9688%\n",
      "----- 1600th batch | Train Acc: 95.2031%\n",
      "----- 1800th batch | Train Acc: 95.5156%\n",
      "Epoch: 19 | Training Loss: 0.1281 | Val Loss: 0.1412 | Train Acc: 95.26% | Val Acc: 95.10%\n",
      "----- 0200th batch | Train Acc: 95.4844%\n",
      "----- 0400th batch | Train Acc: 95.3750%\n",
      "----- 0600th batch | Train Acc: 95.3828%\n",
      "----- 0800th batch | Train Acc: 95.4141%\n",
      "----- 1000th batch | Train Acc: 95.3047%\n",
      "----- 1200th batch | Train Acc: 95.3984%\n",
      "----- 1400th batch | Train Acc: 95.2891%\n",
      "----- 1600th batch | Train Acc: 95.1328%\n",
      "----- 1800th batch | Train Acc: 95.0469%\n",
      "Epoch: 20 | Training Loss: 0.1254 | Val Loss: 0.1470 | Train Acc: 95.29% | Val Acc: 94.66%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "----- 0200th batch | Train Acc: 95.7031%\n",
      "----- 0400th batch | Train Acc: 95.2188%\n",
      "----- 0600th batch | Train Acc: 95.4297%\n",
      "----- 0800th batch | Train Acc: 95.2266%\n",
      "----- 1000th batch | Train Acc: 95.2812%\n",
      "----- 1200th batch | Train Acc: 95.4609%\n",
      "----- 1400th batch | Train Acc: 95.1406%\n",
      "----- 1600th batch | Train Acc: 95.5938%\n",
      "----- 1800th batch | Train Acc: 95.4844%\n",
      "Epoch: 21 | Training Loss: 0.1226 | Val Loss: 0.1514 | Train Acc: 95.40% | Val Acc: 94.92%\n",
      "EarlyStopping counter: 2 out of 10\n",
      "----- 0200th batch | Train Acc: 95.4766%\n",
      "----- 0400th batch | Train Acc: 95.7891%\n",
      "----- 0600th batch | Train Acc: 95.6875%\n",
      "----- 0800th batch | Train Acc: 95.3906%\n",
      "----- 1000th batch | Train Acc: 94.9531%\n",
      "----- 1200th batch | Train Acc: 95.4453%\n",
      "----- 1400th batch | Train Acc: 95.3438%\n",
      "----- 1600th batch | Train Acc: 95.2344%\n",
      "----- 1800th batch | Train Acc: 95.6797%\n",
      "Epoch: 22 | Training Loss: 0.1203 | Val Loss: 0.1515 | Train Acc: 95.46% | Val Acc: 94.60%\n",
      "EarlyStopping counter: 3 out of 10\n",
      "----- 0200th batch | Train Acc: 95.6328%\n",
      "----- 0400th batch | Train Acc: 95.6094%\n",
      "----- 0600th batch | Train Acc: 95.2891%\n",
      "----- 0800th batch | Train Acc: 95.7656%\n",
      "----- 1000th batch | Train Acc: 95.4766%\n",
      "----- 1200th batch | Train Acc: 95.3750%\n",
      "----- 1400th batch | Train Acc: 95.4688%\n",
      "----- 1600th batch | Train Acc: 95.1641%\n",
      "----- 1800th batch | Train Acc: 95.3984%\n",
      "Epoch: 23 | Training Loss: 0.1189 | Val Loss: 0.1464 | Train Acc: 95.47% | Val Acc: 94.99%\n",
      "EarlyStopping counter: 4 out of 10\n",
      "----- 0200th batch | Train Acc: 95.6094%\n",
      "----- 0400th batch | Train Acc: 95.7266%\n",
      "----- 0600th batch | Train Acc: 95.8047%\n",
      "----- 0800th batch | Train Acc: 95.2188%\n",
      "----- 1000th batch | Train Acc: 95.5000%\n",
      "----- 1200th batch | Train Acc: 95.7031%\n",
      "----- 1400th batch | Train Acc: 95.4844%\n",
      "----- 1600th batch | Train Acc: 95.7734%\n",
      "----- 1800th batch | Train Acc: 95.3516%\n",
      "Epoch: 24 | Training Loss: 0.1158 | Val Loss: 0.1409 | Train Acc: 95.60% | Val Acc: 95.28%\n",
      "----- 0200th batch | Train Acc: 95.7422%\n",
      "----- 0400th batch | Train Acc: 95.8516%\n",
      "----- 0600th batch | Train Acc: 95.4766%\n",
      "----- 0800th batch | Train Acc: 95.9297%\n",
      "----- 1000th batch | Train Acc: 95.6875%\n",
      "----- 1200th batch | Train Acc: 96.0547%\n",
      "----- 1400th batch | Train Acc: 95.2578%\n",
      "----- 1600th batch | Train Acc: 95.5391%\n",
      "----- 1800th batch | Train Acc: 95.5156%\n",
      "Epoch: 25 | Training Loss: 0.1147 | Val Loss: 0.1365 | Train Acc: 95.68% | Val Acc: 95.42%\n",
      "----- 0200th batch | Train Acc: 95.7109%\n",
      "----- 0400th batch | Train Acc: 95.7266%\n",
      "----- 0600th batch | Train Acc: 95.4453%\n",
      "----- 0800th batch | Train Acc: 95.8438%\n",
      "----- 1000th batch | Train Acc: 95.5000%\n",
      "----- 1200th batch | Train Acc: 95.6250%\n",
      "----- 1400th batch | Train Acc: 95.7109%\n",
      "----- 1600th batch | Train Acc: 95.8125%\n",
      "----- 1800th batch | Train Acc: 95.6016%\n",
      "Epoch: 26 | Training Loss: 0.1128 | Val Loss: 0.1403 | Train Acc: 95.67% | Val Acc: 95.25%\n",
      "EarlyStopping counter: 1 out of 10\n",
      "----- 0200th batch | Train Acc: 95.9219%\n",
      "----- 0400th batch | Train Acc: 95.8906%\n",
      "----- 0600th batch | Train Acc: 95.8281%\n",
      "----- 0800th batch | Train Acc: 95.8359%\n",
      "----- 1000th batch | Train Acc: 95.2969%\n",
      "----- 1200th batch | Train Acc: 95.9297%\n",
      "----- 1400th batch | Train Acc: 95.7734%\n",
      "----- 1600th batch | Train Acc: 95.6641%\n",
      "----- 1800th batch | Train Acc: 95.6719%\n",
      "Epoch: 27 | Training Loss: 0.1118 | Val Loss: 0.1471 | Train Acc: 95.77% | Val Acc: 94.86%\n",
      "EarlyStopping counter: 2 out of 10\n",
      "----- 0200th batch | Train Acc: 96.0078%\n",
      "----- 0400th batch | Train Acc: 95.9766%\n",
      "----- 0600th batch | Train Acc: 95.6875%\n",
      "----- 0800th batch | Train Acc: 95.7812%\n",
      "----- 1000th batch | Train Acc: 95.8516%\n",
      "----- 1200th batch | Train Acc: 95.8125%\n",
      "----- 1400th batch | Train Acc: 95.7656%\n",
      "----- 1600th batch | Train Acc: 95.6406%\n",
      "----- 1800th batch | Train Acc: 95.5938%\n",
      "Epoch: 28 | Training Loss: 0.1107 | Val Loss: 0.1435 | Train Acc: 95.76% | Val Acc: 95.20%\n",
      "EarlyStopping counter: 3 out of 10\n",
      "----- 0200th batch | Train Acc: 95.6484%\n",
      "----- 0400th batch | Train Acc: 95.8594%\n",
      "----- 0600th batch | Train Acc: 95.8438%\n",
      "----- 0800th batch | Train Acc: 95.8438%\n",
      "----- 1000th batch | Train Acc: 95.8438%\n",
      "----- 1200th batch | Train Acc: 96.0625%\n",
      "----- 1400th batch | Train Acc: 95.6328%\n",
      "----- 1600th batch | Train Acc: 96.0156%\n",
      "----- 1800th batch | Train Acc: 95.9297%\n",
      "Epoch: 29 | Training Loss: 0.1079 | Val Loss: 0.1360 | Train Acc: 95.84% | Val Acc: 95.14%\n",
      "EarlyStopping counter: 4 out of 10\n",
      "----- 0200th batch | Train Acc: 96.0000%\n",
      "----- 0400th batch | Train Acc: 95.7578%\n",
      "----- 0600th batch | Train Acc: 95.8828%\n",
      "----- 0800th batch | Train Acc: 95.9141%\n",
      "----- 1000th batch | Train Acc: 95.9844%\n",
      "----- 1200th batch | Train Acc: 95.9375%\n",
      "----- 1400th batch | Train Acc: 95.8906%\n",
      "----- 1600th batch | Train Acc: 95.8828%\n",
      "----- 1800th batch | Train Acc: 95.8984%\n",
      "Epoch: 30 | Training Loss: 0.1053 | Val Loss: 0.1448 | Train Acc: 95.88% | Val Acc: 95.24%\n",
      "EarlyStopping counter: 5 out of 10\n",
      "----- 0200th batch | Train Acc: 95.7031%\n",
      "----- 0400th batch | Train Acc: 95.8984%\n",
      "----- 0600th batch | Train Acc: 96.1328%\n",
      "----- 0800th batch | Train Acc: 95.9766%\n",
      "----- 1000th batch | Train Acc: 96.2578%\n",
      "----- 1200th batch | Train Acc: 95.8047%\n",
      "----- 1400th batch | Train Acc: 95.7031%\n",
      "----- 1600th batch | Train Acc: 95.8594%\n",
      "----- 1800th batch | Train Acc: 95.7266%\n",
      "Epoch: 31 | Training Loss: 0.1052 | Val Loss: 0.1463 | Train Acc: 95.90% | Val Acc: 95.07%\n",
      "EarlyStopping counter: 6 out of 10\n",
      "----- 0200th batch | Train Acc: 95.7734%\n",
      "----- 0400th batch | Train Acc: 96.0781%\n",
      "----- 0600th batch | Train Acc: 95.9922%\n",
      "----- 0800th batch | Train Acc: 96.0078%\n",
      "----- 1000th batch | Train Acc: 96.1406%\n",
      "----- 1200th batch | Train Acc: 95.9531%\n",
      "----- 1400th batch | Train Acc: 96.1250%\n",
      "----- 1600th batch | Train Acc: 96.2188%\n",
      "----- 1800th batch | Train Acc: 96.0078%\n",
      "Epoch: 32 | Training Loss: 0.1025 | Val Loss: 0.1448 | Train Acc: 96.02% | Val Acc: 95.04%\n",
      "EarlyStopping counter: 7 out of 10\n",
      "----- 0200th batch | Train Acc: 96.2344%\n",
      "----- 0400th batch | Train Acc: 95.9453%\n",
      "----- 0600th batch | Train Acc: 96.0547%\n",
      "----- 0800th batch | Train Acc: 96.2344%\n",
      "----- 1000th batch | Train Acc: 96.1172%\n",
      "----- 1200th batch | Train Acc: 96.0312%\n",
      "----- 1400th batch | Train Acc: 96.1797%\n",
      "----- 1600th batch | Train Acc: 95.9531%\n",
      "----- 1800th batch | Train Acc: 95.8594%\n",
      "Epoch: 33 | Training Loss: 0.1018 | Val Loss: 0.1439 | Train Acc: 96.06% | Val Acc: 94.96%\n",
      "EarlyStopping counter: 8 out of 10\n",
      "----- 0200th batch | Train Acc: 96.0781%\n",
      "----- 0400th batch | Train Acc: 96.1719%\n",
      "----- 0600th batch | Train Acc: 95.9766%\n",
      "----- 0800th batch | Train Acc: 95.8984%\n",
      "----- 1000th batch | Train Acc: 95.7109%\n",
      "----- 1200th batch | Train Acc: 96.1250%\n",
      "----- 1400th batch | Train Acc: 96.1953%\n",
      "----- 1600th batch | Train Acc: 96.0547%\n",
      "----- 1800th batch | Train Acc: 95.9688%\n",
      "Epoch: 34 | Training Loss: 0.1017 | Val Loss: 0.1450 | Train Acc: 96.04% | Val Acc: 95.20%\n",
      "EarlyStopping counter: 9 out of 10\n",
      "----- 0200th batch | Train Acc: 96.3281%\n",
      "----- 0400th batch | Train Acc: 95.9609%\n",
      "----- 0600th batch | Train Acc: 95.9531%\n",
      "----- 0800th batch | Train Acc: 96.0469%\n",
      "----- 1000th batch | Train Acc: 96.0156%\n",
      "----- 1200th batch | Train Acc: 96.1094%\n",
      "----- 1400th batch | Train Acc: 96.5312%\n",
      "----- 1600th batch | Train Acc: 96.1719%\n",
      "----- 1800th batch | Train Acc: 96.0469%\n",
      "Epoch: 35 | Training Loss: 0.0996 | Val Loss: 0.1529 | Train Acc: 96.13% | Val Acc: 94.54%\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping condition met --- TRAINING STOPPED\n"
     ]
    }
   ],
   "source": [
    "model_fit = train_model(model, epochs, learning_rate, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=26, bias=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fit.block[0].fc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.fc = nn.Linear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "dacon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
