{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from utils.imageprocess import image_transformer, image_processor\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from utils.dataloader import CustomDataLoader\n",
    "from utils.radams import RAdam\n",
    "\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import PlainResnet50, CustomResnet50, PlainEfficientnetB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format=\"%(asctime)s : %(message)s\", \n",
    "                    level=logging.INFO,\n",
    "                    handlers=[\n",
    "                        logging.FileHandler('./logs/pretraining_log'),\n",
    "                        logging.StreamHandler()\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopper\n",
    "from utils.EarlyStopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_dict = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((40, 40)),\n",
    "        transforms.Pad(108, fill=0, padding_mode='constant'),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        \n",
    "        transforms.RandomPerspective(distortion_scale=0.3, p=0.5),\n",
    "        transforms.RandomAffine(degrees=0,\n",
    "                                translate=(0.3, 0.3),\n",
    "                                fillcolor=0),\n",
    "        transforms.RandomAffine(degrees=0,\n",
    "                                shear=(0, 10, 0, 10)),\n",
    "        \n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((40, 40)),\n",
    "        transforms.Pad(108, fill=0, padding_mode='constant'),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "}\n",
    "\n",
    "trainset = torchvision.datasets.EMNIST(root='./dataset/emnist_dataset',\n",
    "                                       split='letters',\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=transform_dict['train'])\n",
    "\n",
    "testset = torchvision.datasets.EMNIST(root='./dataset/emnist_dataset',\n",
    "                                      split='letters',\n",
    "                                      train=False,\n",
    "                                      download=True,\n",
    "                                      transform=transform_dict['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sks/anaconda3/envs/dacon/lib/python3.8/site-packages/torchvision/transforms/functional_tensor.py:876: UserWarning: Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\n",
      "  warnings.warn(\"Argument fill/fillcolor is not supported for Tensor input. Fill value is zero\")\n",
      "2021-02-12 13:01:05,219 : Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f725c26db20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainset[2][0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = 0.8\n",
    "# train_len = int(len(trainset) * train_ratio)\n",
    "# val_len = len(trainset) - train_len\n",
    "# logger.info(f'Train Length: {train_len}, Validation Length: {val_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, val_set = torch.utils.data.random_split(trainset, lengths=[train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "    elif classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "def train_model(model=None, epochs=50, learning_rate=0.002, *loaders):\n",
    "    \n",
    "    #model.apply(weights_init)\n",
    "    \n",
    "    # Dataloaders (train/val)\n",
    "    train_loader = loaders[0]\n",
    "    val_loader = loaders[1]\n",
    "    \n",
    "    # Early Stopper\n",
    "    early_stopping = EarlyStopping(patience=7, verbose=False, path='./pretrained_model/plain_efficientnetb4_ckpt.pth')\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = RAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "    \n",
    "    \n",
    "    #lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=epochs/4)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer,\n",
    "                                                        milestones=[int(epochs*0.3), int(epochs*0.6)],\n",
    "                                                        gamma=0.5)\n",
    "    \n",
    "    warmup_epochs = int(epochs * 0.15)\n",
    "    lr_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=warmup_epochs, after_scheduler=lr_scheduler)\n",
    "    #decayRate = 0.998\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "    \n",
    "    # amp\n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    \n",
    "    train_tot_num = train_loader.dataset.__len__()\n",
    "    val_tot_num = val_loader.dataset.__len__()\n",
    "    \n",
    "    train_loss_sum, train_acc_sum = 0.0, 0.0\n",
    "    val_loss_sum, val_acc_sum = 0.0, 0.0\n",
    "    \n",
    "    logger.info(f'Training begins: epochs={epochs}')\n",
    "    for epoch in range(epochs):\n",
    "        logger.info(f'Epoch: {epoch+1}')\n",
    "        if epoch <= warmup_epochs:\n",
    "            lr_warmup.step()\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        \n",
    "        logger.info(f\"Learning Rate : {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        train_tmp_corrects_sum, train_tmp_len = 0, 0\n",
    "        for idx, (train_X, train_Y) in enumerate(train_loader):    \n",
    "            \n",
    "            train_tmp_len += len(train_Y)\n",
    "            \n",
    "            train_X = train_X.to(device)\n",
    "            train_Y = train_Y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with amp.autocast():\n",
    "                train_pred = model(train_X)\n",
    "                train_loss = loss_function(train_pred, train_Y-1)\n",
    "            \n",
    "            scaler.scale(train_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # Accuracy\n",
    "            pred_labels = torch.argmax(train_pred, axis=1)\n",
    "            train_acc = (pred_labels==(train_Y-1)).sum()\n",
    "            \n",
    "            train_acc_sum += train_acc.item()\n",
    "            train_loss_sum += train_loss.item()\n",
    "            \n",
    "            train_tmp_corrects_sum += train_acc.item()\n",
    "            \n",
    "            # Check between batches\n",
    "            if (idx+1) % 500 == 0:\n",
    "                print(f\"----- {str((idx+1)).zfill(5)} / {len(train_loader)}th batch | Train Acc: {train_tmp_corrects_sum/train_tmp_len*100:.4f}%\")\n",
    "                \n",
    "                # initialization\n",
    "                train_tmp_corrects_sum, train_tmp_len = 0, 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    " \n",
    "            for idx, (val_X, val_Y) in enumerate(val_loader):\n",
    "\n",
    "                val_X, val_Y = val_X.to(device), val_Y.to(device)\n",
    "                val_pred = model(val_X)\n",
    "                val_loss = loss_function(val_pred, val_Y-1)\n",
    "                \n",
    "                val_pred_labels = torch.argmax(val_pred, axis=1)\n",
    "                val_acc = (val_pred_labels==(val_Y-1)).sum()\n",
    "                \n",
    "                val_acc_sum += val_acc.item()\n",
    "                val_loss_sum += val_loss.item()\n",
    "        \n",
    "        train_accuracy = train_acc_sum / train_tot_num * 100\n",
    "        val_accuracy = val_acc_sum / val_tot_num * 100\n",
    "        \n",
    "        logger.info(f\"Epoch: {epoch+1} | Training Loss: {train_loss_sum/train_tot_num:.4f} | Val Loss: {val_loss_sum/val_tot_num:.4f} | \" \\\n",
    "              f\"Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        train_loss_sum, train_acc_sum = 0.0, 0.0\n",
    "        val_loss_sum, val_acc_sum = 0.0, 0.0\n",
    "        \n",
    "        early_stopping(-val_accuracy, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            logger.info(\"Early stopping condition met --- TRAINING STOPPED\")\n",
    "            break\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "# resnet_model = resnet50().to(device)\n",
    "# resnet_model.fc = nn.Sequential(\n",
    "#     nn.Linear(2048, 256),\n",
    "#     nn.BatchNorm1d(256),\n",
    "    \n",
    "#     nn.Linear(256, 26),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "# 모델 선언\n",
    "model = PlainEfficientnetB4().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 13:04:45,386 : Training begins: epochs=50\n",
      "2021-02-12 13:04:45,386 : Epoch: 1\n",
      "/home/sks/anaconda3/envs/dacon/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "2021-02-12 13:04:45,388 : Learning Rate : 0.000357\n",
      "/home/sks/COMPETITION/DACON/computer_vision2/utils/radams.py:45: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 50.8938%\n",
      "----- 01000 / 3900th batch | Train Acc: 83.6125%\n",
      "----- 01500 / 3900th batch | Train Acc: 87.9688%\n",
      "----- 02000 / 3900th batch | Train Acc: 88.9375%\n",
      "----- 02500 / 3900th batch | Train Acc: 89.4062%\n",
      "----- 03000 / 3900th batch | Train Acc: 90.7000%\n",
      "----- 03500 / 3900th batch | Train Acc: 90.9813%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 13:27:13,012 : Epoch: 1 | Training Loss: 0.0159 | Val Loss: 0.0074 | Train Acc: 84.06% | Val Acc: 91.73%\n",
      "2021-02-12 13:27:13,105 : Epoch: 2\n",
      "2021-02-12 13:27:13,224 : Learning Rate : 0.000714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 88.7563%\n",
      "----- 01000 / 3900th batch | Train Acc: 89.9000%\n",
      "----- 01500 / 3900th batch | Train Acc: 90.0875%\n",
      "----- 02000 / 3900th batch | Train Acc: 91.3375%\n",
      "----- 02500 / 3900th batch | Train Acc: 91.2937%\n",
      "----- 03000 / 3900th batch | Train Acc: 91.8875%\n",
      "----- 03500 / 3900th batch | Train Acc: 91.5812%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 13:49:44,858 : Epoch: 2 | Training Loss: 0.0087 | Val Loss: 0.0068 | Train Acc: 90.84% | Val Acc: 92.67%\n",
      "2021-02-12 13:49:45,069 : Epoch: 3\n",
      "2021-02-12 13:49:45,211 : Learning Rate : 0.001071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 90.5062%\n",
      "----- 01000 / 3900th batch | Train Acc: 90.6937%\n",
      "----- 01500 / 3900th batch | Train Acc: 91.3188%\n",
      "----- 02000 / 3900th batch | Train Acc: 92.0500%\n",
      "----- 02500 / 3900th batch | Train Acc: 91.7562%\n",
      "----- 03000 / 3900th batch | Train Acc: 91.8562%\n",
      "----- 03500 / 3900th batch | Train Acc: 92.0062%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 14:12:15,973 : Epoch: 3 | Training Loss: 0.0080 | Val Loss: 0.0078 | Train Acc: 91.51% | Val Acc: 91.87%\n",
      "2021-02-12 14:12:15,974 : Epoch: 4\n",
      "2021-02-12 14:12:16,097 : Learning Rate : 0.001429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 91.4937%\n",
      "----- 01000 / 3900th batch | Train Acc: 91.2250%\n",
      "----- 01500 / 3900th batch | Train Acc: 91.7250%\n",
      "----- 02000 / 3900th batch | Train Acc: 92.0625%\n",
      "----- 02500 / 3900th batch | Train Acc: 92.4750%\n",
      "----- 03000 / 3900th batch | Train Acc: 92.0125%\n",
      "----- 03500 / 3900th batch | Train Acc: 92.1625%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 14:34:47,741 : Epoch: 4 | Training Loss: 0.0076 | Val Loss: 0.0069 | Train Acc: 91.95% | Val Acc: 92.38%\n",
      "2021-02-12 14:34:47,742 : Epoch: 5\n",
      "2021-02-12 14:34:47,867 : Learning Rate : 0.001786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 91.5000%\n",
      "----- 01000 / 3900th batch | Train Acc: 91.8000%\n",
      "----- 01500 / 3900th batch | Train Acc: 92.4813%\n",
      "----- 02000 / 3900th batch | Train Acc: 91.8750%\n",
      "----- 02500 / 3900th batch | Train Acc: 92.2562%\n",
      "----- 03000 / 3900th batch | Train Acc: 92.6188%\n",
      "----- 03500 / 3900th batch | Train Acc: 92.6125%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 14:57:19,787 : Epoch: 5 | Training Loss: 0.0074 | Val Loss: 0.0063 | Train Acc: 92.21% | Val Acc: 93.39%\n",
      "2021-02-12 14:57:20,059 : Epoch: 6\n",
      "2021-02-12 14:57:20,221 : Learning Rate : 0.002143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 92.0312%\n",
      "----- 01000 / 3900th batch | Train Acc: 92.4750%\n",
      "----- 01500 / 3900th batch | Train Acc: 92.1125%\n",
      "----- 02000 / 3900th batch | Train Acc: 92.3750%\n",
      "----- 02500 / 3900th batch | Train Acc: 92.1500%\n",
      "----- 03000 / 3900th batch | Train Acc: 92.7875%\n",
      "----- 03500 / 3900th batch | Train Acc: 93.0875%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 15:19:53,413 : Epoch: 6 | Training Loss: 0.0071 | Val Loss: 0.0061 | Train Acc: 92.43% | Val Acc: 93.65%\n",
      "2021-02-12 15:19:53,637 : Epoch: 7\n",
      "2021-02-12 15:19:53,787 : Learning Rate : 0.002500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 92.6312%\n",
      "----- 01000 / 3900th batch | Train Acc: 92.2687%\n",
      "----- 01500 / 3900th batch | Train Acc: 92.8125%\n",
      "----- 02000 / 3900th batch | Train Acc: 92.5375%\n",
      "----- 02500 / 3900th batch | Train Acc: 92.5875%\n",
      "----- 03000 / 3900th batch | Train Acc: 92.7687%\n",
      "----- 03500 / 3900th batch | Train Acc: 93.1625%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 15:42:24,866 : Epoch: 7 | Training Loss: 0.0069 | Val Loss: 0.0076 | Train Acc: 92.69% | Val Acc: 91.71%\n",
      "2021-02-12 15:42:24,867 : Epoch: 8\n",
      "2021-02-12 15:42:25,007 : Learning Rate : 0.001912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 93.6125%\n",
      "----- 01000 / 3900th batch | Train Acc: 93.6750%\n",
      "----- 01500 / 3900th batch | Train Acc: 93.7438%\n",
      "----- 02000 / 3900th batch | Train Acc: 93.3438%\n",
      "----- 02500 / 3900th batch | Train Acc: 93.4313%\n",
      "----- 03000 / 3900th batch | Train Acc: 93.5438%\n",
      "----- 03500 / 3900th batch | Train Acc: 93.6187%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 16:04:53,973 : Epoch: 8 | Training Loss: 0.0059 | Val Loss: 0.0060 | Train Acc: 93.58% | Val Acc: 93.69%\n",
      "2021-02-12 16:04:54,204 : Epoch: 9\n",
      "2021-02-12 16:04:54,329 : Learning Rate : 0.001351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 94.3563%\n",
      "----- 01000 / 3900th batch | Train Acc: 94.3750%\n",
      "----- 01500 / 3900th batch | Train Acc: 94.3250%\n",
      "----- 02000 / 3900th batch | Train Acc: 94.4062%\n",
      "----- 02500 / 3900th batch | Train Acc: 94.5063%\n",
      "----- 03000 / 3900th batch | Train Acc: 94.1688%\n",
      "----- 03500 / 3900th batch | Train Acc: 94.2375%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 16:27:22,697 : Epoch: 9 | Training Loss: 0.0051 | Val Loss: 0.0052 | Train Acc: 94.36% | Val Acc: 94.38%\n",
      "2021-02-12 16:27:22,919 : Epoch: 10\n",
      "2021-02-12 16:27:23,066 : Learning Rate : 0.000853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 95.0438%\n",
      "----- 01000 / 3900th batch | Train Acc: 94.9062%\n",
      "----- 01500 / 3900th batch | Train Acc: 94.6937%\n",
      "----- 02000 / 3900th batch | Train Acc: 95.0625%\n",
      "----- 02500 / 3900th batch | Train Acc: 95.3812%\n",
      "----- 03000 / 3900th batch | Train Acc: 94.9125%\n",
      "----- 03500 / 3900th batch | Train Acc: 94.9250%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 16:49:51,889 : Epoch: 10 | Training Loss: 0.0044 | Val Loss: 0.0047 | Train Acc: 94.96% | Val Acc: 94.75%\n",
      "2021-02-12 16:49:52,116 : Epoch: 11\n",
      "2021-02-12 16:49:52,269 : Learning Rate : 0.000449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 95.1250%\n",
      "----- 01000 / 3900th batch | Train Acc: 95.5375%\n",
      "----- 01500 / 3900th batch | Train Acc: 95.5438%\n",
      "----- 02000 / 3900th batch | Train Acc: 95.4000%\n",
      "----- 02500 / 3900th batch | Train Acc: 95.2938%\n",
      "----- 03000 / 3900th batch | Train Acc: 95.5562%\n",
      "----- 03500 / 3900th batch | Train Acc: 95.4438%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 17:12:20,947 : Epoch: 11 | Training Loss: 0.0039 | Val Loss: 0.0044 | Train Acc: 95.42% | Val Acc: 95.05%\n",
      "2021-02-12 17:12:21,173 : Epoch: 12\n",
      "2021-02-12 17:12:21,322 : Learning Rate : 0.000165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 95.7500%\n",
      "----- 01000 / 3900th batch | Train Acc: 95.7188%\n",
      "----- 01500 / 3900th batch | Train Acc: 95.5187%\n",
      "----- 02000 / 3900th batch | Train Acc: 95.6812%\n",
      "----- 02500 / 3900th batch | Train Acc: 95.9000%\n",
      "----- 03000 / 3900th batch | Train Acc: 95.9438%\n",
      "----- 03500 / 3900th batch | Train Acc: 95.8625%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 17:34:51,400 : Epoch: 12 | Training Loss: 0.0035 | Val Loss: 0.0042 | Train Acc: 95.77% | Val Acc: 95.35%\n",
      "2021-02-12 17:34:51,644 : Epoch: 13\n",
      "2021-02-12 17:34:51,768 : Learning Rate : 0.000019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 95.9562%\n",
      "----- 01000 / 3900th batch | Train Acc: 95.9062%\n",
      "----- 01500 / 3900th batch | Train Acc: 96.1000%\n",
      "----- 02000 / 3900th batch | Train Acc: 96.1000%\n",
      "----- 02500 / 3900th batch | Train Acc: 95.8937%\n",
      "----- 03000 / 3900th batch | Train Acc: 96.0750%\n",
      "----- 03500 / 3900th batch | Train Acc: 96.0000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 17:57:17,921 : Epoch: 13 | Training Loss: 0.0033 | Val Loss: 0.0041 | Train Acc: 96.02% | Val Acc: 95.32%\n",
      "2021-02-12 17:57:17,921 : Epoch: 14\n",
      "2021-02-12 17:57:18,073 : Learning Rate : 0.000019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 95.8688%\n",
      "----- 01000 / 3900th batch | Train Acc: 96.0563%\n",
      "----- 01500 / 3900th batch | Train Acc: 95.9750%\n",
      "----- 02000 / 3900th batch | Train Acc: 96.0063%\n",
      "----- 02500 / 3900th batch | Train Acc: 96.1000%\n",
      "----- 03000 / 3900th batch | Train Acc: 96.0187%\n",
      "----- 03500 / 3900th batch | Train Acc: 96.2313%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 18:19:47,971 : Epoch: 14 | Training Loss: 0.0033 | Val Loss: 0.0041 | Train Acc: 96.03% | Val Acc: 95.47%\n",
      "2021-02-12 18:19:48,197 : Epoch: 15\n",
      "2021-02-12 18:19:48,360 : Learning Rate : 0.000165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 00500 / 3900th batch | Train Acc: 95.6875%\n",
      "----- 01000 / 3900th batch | Train Acc: 96.0187%\n",
      "----- 01500 / 3900th batch | Train Acc: 95.8375%\n",
      "----- 02000 / 3900th batch | Train Acc: 95.9313%\n",
      "----- 02500 / 3900th batch | Train Acc: 95.9375%\n",
      "----- 03000 / 3900th batch | Train Acc: 95.8063%\n",
      "----- 03500 / 3900th batch | Train Acc: 95.9812%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 18:42:17,903 : Epoch: 15 | Training Loss: 0.0034 | Val Loss: 0.0041 | Train Acc: 95.89% | Val Acc: 95.39%\n",
      "2021-02-12 18:42:17,903 : Epoch: 16\n",
      "2021-02-12 18:42:18,030 : Learning Rate : 0.000449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 1 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 95.8187%\n",
      "----- 01000 / 3900th batch | Train Acc: 95.5375%\n",
      "----- 01500 / 3900th batch | Train Acc: 95.3688%\n",
      "----- 02000 / 3900th batch | Train Acc: 95.7250%\n",
      "----- 02500 / 3900th batch | Train Acc: 95.5500%\n",
      "----- 03000 / 3900th batch | Train Acc: 95.3937%\n",
      "----- 03500 / 3900th batch | Train Acc: 95.8750%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 19:04:47,619 : Epoch: 16 | Training Loss: 0.0036 | Val Loss: 0.0042 | Train Acc: 95.60% | Val Acc: 95.29%\n",
      "2021-02-12 19:04:47,619 : Epoch: 17\n",
      "2021-02-12 19:04:47,745 : Learning Rate : 0.000853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 2 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 95.5063%\n",
      "----- 01000 / 3900th batch | Train Acc: 95.5562%\n",
      "----- 01500 / 3900th batch | Train Acc: 95.4188%\n",
      "----- 02000 / 3900th batch | Train Acc: 95.2250%\n",
      "----- 02500 / 3900th batch | Train Acc: 95.0688%\n",
      "----- 03000 / 3900th batch | Train Acc: 95.2313%\n",
      "----- 03500 / 3900th batch | Train Acc: 95.3500%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 19:28:16,789 : Epoch: 17 | Training Loss: 0.0040 | Val Loss: 0.0045 | Train Acc: 95.31% | Val Acc: 94.99%\n",
      "2021-02-12 19:28:16,790 : Epoch: 18\n",
      "2021-02-12 19:28:16,956 : Learning Rate : 0.001351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 3 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 94.8875%\n",
      "----- 01000 / 3900th batch | Train Acc: 94.6125%\n",
      "----- 01500 / 3900th batch | Train Acc: 94.9000%\n",
      "----- 02000 / 3900th batch | Train Acc: 94.9438%\n",
      "----- 02500 / 3900th batch | Train Acc: 94.9062%\n",
      "----- 03000 / 3900th batch | Train Acc: 94.8125%\n",
      "----- 03500 / 3900th batch | Train Acc: 94.5625%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 19:53:04,250 : Epoch: 18 | Training Loss: 0.0045 | Val Loss: 0.0053 | Train Acc: 94.84% | Val Acc: 94.37%\n",
      "2021-02-12 19:53:04,251 : Epoch: 19\n",
      "2021-02-12 19:53:04,385 : Learning Rate : 0.001912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 4 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 94.5438%\n",
      "----- 01000 / 3900th batch | Train Acc: 93.6875%\n",
      "----- 01500 / 3900th batch | Train Acc: 94.3625%\n",
      "----- 02000 / 3900th batch | Train Acc: 94.6000%\n",
      "----- 02500 / 3900th batch | Train Acc: 94.1000%\n",
      "----- 03000 / 3900th batch | Train Acc: 93.9187%\n",
      "----- 03500 / 3900th batch | Train Acc: 94.5187%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 20:17:56,796 : Epoch: 19 | Training Loss: 0.0050 | Val Loss: 0.0056 | Train Acc: 94.29% | Val Acc: 94.07%\n",
      "2021-02-12 20:17:56,797 : Epoch: 20\n",
      "2021-02-12 20:17:56,972 : Learning Rate : 0.002500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 5 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 94.0438%\n",
      "----- 01000 / 3900th batch | Train Acc: 93.7875%\n",
      "----- 01500 / 3900th batch | Train Acc: 93.8438%\n",
      "----- 02000 / 3900th batch | Train Acc: 93.9250%\n",
      "----- 02500 / 3900th batch | Train Acc: 93.9500%\n",
      "----- 03000 / 3900th batch | Train Acc: 93.9187%\n",
      "----- 03500 / 3900th batch | Train Acc: 93.5875%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 20:42:50,686 : Epoch: 20 | Training Loss: 0.0056 | Val Loss: 0.0055 | Train Acc: 93.88% | Val Acc: 94.04%\n",
      "2021-02-12 20:42:50,687 : Epoch: 21\n",
      "2021-02-12 20:42:50,849 : Learning Rate : 0.003079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 6 out of 7\n",
      "----- 00500 / 3900th batch | Train Acc: 93.5813%\n",
      "----- 01000 / 3900th batch | Train Acc: 93.2562%\n",
      "----- 01500 / 3900th batch | Train Acc: 93.0938%\n",
      "----- 02000 / 3900th batch | Train Acc: 93.9813%\n",
      "----- 02500 / 3900th batch | Train Acc: 93.5562%\n",
      "----- 03000 / 3900th batch | Train Acc: 93.6063%\n",
      "----- 03500 / 3900th batch | Train Acc: 93.4562%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-12 21:07:41,538 : Epoch: 21 | Training Loss: 0.0059 | Val Loss: 0.0057 | Train Acc: 93.47% | Val Acc: 93.91%\n",
      "2021-02-12 21:07:41,539 : Early stopping condition met --- TRAINING STOPPED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 7 out of 7\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0025\n",
    "epochs = 50\n",
    "\n",
    "model_fit = train_model(model, epochs, learning_rate, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.fc = nn.Linear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "dacon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
