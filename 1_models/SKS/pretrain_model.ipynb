{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from src.model import DaconModel\n",
    "\n",
    "from utils.imageprocess import image_transformer, image_processor\n",
    "from utils.EarlyStopping import EarlyStopping\n",
    "from utils.dataloader import CustomDataLoader\n",
    "from utils.radams import RAdam\n",
    "\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format='%(asctime)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopper\n",
    "from utils.EarlyStopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Current Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform_dict = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((40, 40)),\n",
    "        transforms.RandomChoice([transforms.RandomRotation(30, expand=False), \n",
    "                                 transforms.RandomPerspective()]),\n",
    "        transforms.Pad(108, fill=0, padding_mode='constant'),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((40, 40)),\n",
    "        transforms.Pad(108, fill=0, padding_mode='constant'),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "}\n",
    "\n",
    "trainset = torchvision.datasets.EMNIST(root='./dataset/emnist_dataset',\n",
    "                                       split='letters',\n",
    "                                       train=True,\n",
    "                                       download=True,\n",
    "                                       transform=transform_dict['train'])\n",
    "\n",
    "testset = torchvision.datasets.EMNIST(root='./dataset/emnist_dataset',\n",
    "                                      split='letters',\n",
    "                                      train=False,\n",
    "                                      download=True,\n",
    "                                      transform=transform_dict['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ratio = 0.8\n",
    "# train_len = int(len(trainset) * train_ratio)\n",
    "# val_len = len(trainset) - train_len\n",
    "# logger.info(f'Train Length: {train_len}, Validation Length: {val_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set, val_set = torch.utils.data.random_split(trainset, lengths=[train_len, val_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "#val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "        \n",
    "    elif classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "def train_model(model=None, epochs=50, lr=50, *loaders):\n",
    "    \n",
    "    #model.apply(weights_init)\n",
    "    \n",
    "    # Dataloaders (train/val)\n",
    "    train_loader = loaders[0]\n",
    "    val_loader = loaders[1]\n",
    "    \n",
    "    # Early Stopper\n",
    "    early_stopping = EarlyStopping(patience=3, verbose=False, path='./pretrained_resnet.pth')\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    #optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = RAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "\n",
    "    decayRate = 0.998\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "    \n",
    "    train_tot_num = train_loader.dataset.__len__()\n",
    "    val_tot_num = val_loader.dataset.__len__()\n",
    "    \n",
    "    train_loss_sum, train_acc_sum = 0.0, 0.0\n",
    "    val_loss_sum, val_acc_sum = 0.0, 0.0\n",
    "    \n",
    "    logger.info(f'Training begins: epochs={epochs}')\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        \n",
    "        train_tmp_corrects_sum, train_tmp_len = 0, 0\n",
    "        for idx, (train_X, train_Y) in enumerate(train_loader):    \n",
    "            \n",
    "            train_tmp_len += len(train_Y)\n",
    "            \n",
    "            train_X = train_X.to(device)\n",
    "            train_Y = train_Y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            train_pred = model(train_X)\n",
    "            train_loss = loss_function(train_pred, train_Y-1)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accuracy\n",
    "            pred_labels = torch.argmax(train_pred, axis=1)\n",
    "            train_acc = (pred_labels==(train_Y-1)).sum()\n",
    "            \n",
    "            train_acc_sum += train_acc.item()\n",
    "            train_loss_sum += train_loss.item()\n",
    "            \n",
    "            train_tmp_corrects_sum += train_acc.item()\n",
    "            \n",
    "            # Check between batches\n",
    "            if (idx+1) % 100 == 0:\n",
    "                print(f\"----- {str((idx+1)).zfill(4)}th batch | Train Acc: {train_tmp_corrects_sum/train_tmp_len*100:.4f}%\")\n",
    "                \n",
    "                # initialization\n",
    "                train_tmp_corrects_sum, train_tmp_len = 0, 0\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for idx, (val_X, val_Y) in enumerate(val_loader):\n",
    "                \n",
    "                val_X, val_Y = val_X.to(device), val_Y.to(device)\n",
    "                val_pred = model(val_X)\n",
    "                val_loss = loss_function(val_pred, val_Y-1)\n",
    "                \n",
    "                val_pred_labels = torch.argmax(val_pred, axis=1)\n",
    "                val_acc = (val_pred_labels==(val_Y-1)).sum()\n",
    "                \n",
    "                val_acc_sum += val_acc.item()\n",
    "                val_loss_sum += val_loss.item()\n",
    "        \n",
    "        train_accuracy = train_acc_sum / train_tot_num * 100\n",
    "        val_accuracy = val_acc_sum / val_tot_num * 100\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1} | Training Loss: {train_loss_sum/len(train_loader):.4f} | Val Loss: {val_loss_sum/len(val_loader):.4f} | \" \\\n",
    "              f\"Train Acc: {train_accuracy:.2f}% | Val Acc: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        train_loss_sum, train_acc_sum = 0.0, 0.0\n",
    "        val_loss_sum, val_acc_sum = 0.0, 0.0\n",
    "        \n",
    "        early_stopping(-val_accuracy, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping condition met --- TRAINING STOPPED\")\n",
    "            break\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0037\n",
    "epochs = 50\n",
    "\n",
    "resnet_model = resnet50().to(device)\n",
    "resnet_model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 256),\n",
    "    nn.Linear(256, 26),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PretrainedModel, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, 1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            resnet_model,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "# 모델 선언\n",
    "model = PretrainedModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=256, out_features=26, bias=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.block[2].fc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-04 17:54:20,789 : Training begins: epochs=50\n",
      "/home/sks/COMPETITION/DACON/computer_vision2/utils/radams.py:45: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- 0100th batch | Train Acc: 14.4375%\n",
      "----- 0200th batch | Train Acc: 33.3438%\n",
      "----- 0300th batch | Train Acc: 40.6250%\n",
      "----- 0400th batch | Train Acc: 46.2812%\n",
      "----- 0500th batch | Train Acc: 53.0000%\n",
      "----- 0600th batch | Train Acc: 59.8125%\n",
      "----- 0700th batch | Train Acc: 62.1562%\n",
      "----- 0800th batch | Train Acc: 64.9375%\n",
      "----- 0900th batch | Train Acc: 68.2500%\n",
      "----- 1000th batch | Train Acc: 67.9688%\n",
      "----- 1100th batch | Train Acc: 72.9062%\n",
      "----- 1200th batch | Train Acc: 73.2812%\n",
      "----- 1300th batch | Train Acc: 76.2500%\n",
      "----- 1400th batch | Train Acc: 75.7500%\n",
      "----- 1500th batch | Train Acc: 75.6250%\n",
      "----- 1600th batch | Train Acc: 77.4688%\n",
      "----- 1700th batch | Train Acc: 79.0000%\n",
      "----- 1800th batch | Train Acc: 79.3438%\n",
      "----- 1900th batch | Train Acc: 81.8750%\n",
      "----- 2000th batch | Train Acc: 80.3438%\n",
      "----- 2100th batch | Train Acc: 79.2812%\n",
      "----- 2200th batch | Train Acc: 81.6875%\n",
      "----- 2300th batch | Train Acc: 81.9375%\n",
      "----- 2400th batch | Train Acc: 84.1250%\n",
      "----- 2500th batch | Train Acc: 82.6875%\n",
      "----- 2600th batch | Train Acc: 83.7812%\n",
      "----- 2700th batch | Train Acc: 83.4062%\n",
      "----- 2800th batch | Train Acc: 82.6250%\n",
      "----- 2900th batch | Train Acc: 84.4375%\n",
      "----- 3000th batch | Train Acc: 84.4062%\n",
      "----- 3100th batch | Train Acc: 83.8438%\n",
      "----- 3200th batch | Train Acc: 85.0625%\n",
      "----- 3300th batch | Train Acc: 85.7188%\n",
      "----- 3400th batch | Train Acc: 83.4688%\n",
      "----- 3500th batch | Train Acc: 84.3125%\n",
      "----- 3600th batch | Train Acc: 86.5938%\n",
      "----- 3700th batch | Train Acc: 84.6875%\n",
      "----- 3800th batch | Train Acc: 84.8750%\n",
      "----- 3900th batch | Train Acc: 84.4688%\n",
      "Epoch: 1 | Training Loss: 0.8278 | Val Loss: 0.3723 | Train Acc: 73.69% | Val Acc: 88.01%\n",
      "----- 0100th batch | Train Acc: 86.1250%\n",
      "----- 0200th batch | Train Acc: 85.5000%\n",
      "----- 0300th batch | Train Acc: 86.7500%\n",
      "----- 0400th batch | Train Acc: 86.1250%\n",
      "----- 0500th batch | Train Acc: 85.9375%\n",
      "----- 0600th batch | Train Acc: 85.4375%\n",
      "----- 0700th batch | Train Acc: 87.5312%\n",
      "----- 0800th batch | Train Acc: 86.3125%\n",
      "----- 0900th batch | Train Acc: 86.3125%\n",
      "----- 1000th batch | Train Acc: 87.2500%\n",
      "----- 1100th batch | Train Acc: 87.4375%\n",
      "----- 1200th batch | Train Acc: 87.1875%\n",
      "----- 1300th batch | Train Acc: 87.4688%\n",
      "----- 1400th batch | Train Acc: 87.9375%\n",
      "----- 1500th batch | Train Acc: 87.5625%\n",
      "----- 1600th batch | Train Acc: 86.7812%\n",
      "----- 1700th batch | Train Acc: 88.7500%\n",
      "----- 1800th batch | Train Acc: 87.0938%\n",
      "----- 1900th batch | Train Acc: 87.9375%\n",
      "----- 2000th batch | Train Acc: 87.0000%\n",
      "----- 2100th batch | Train Acc: 88.4688%\n",
      "----- 2200th batch | Train Acc: 88.0312%\n",
      "----- 2300th batch | Train Acc: 88.5312%\n",
      "----- 2400th batch | Train Acc: 87.0312%\n",
      "----- 2500th batch | Train Acc: 87.8750%\n",
      "----- 2600th batch | Train Acc: 87.6250%\n",
      "----- 2700th batch | Train Acc: 87.4062%\n",
      "----- 2800th batch | Train Acc: 88.1875%\n",
      "----- 2900th batch | Train Acc: 87.9062%\n",
      "----- 3000th batch | Train Acc: 87.6250%\n",
      "----- 3100th batch | Train Acc: 89.2188%\n",
      "----- 3200th batch | Train Acc: 89.8438%\n",
      "----- 3300th batch | Train Acc: 89.8438%\n",
      "----- 3400th batch | Train Acc: 88.5625%\n",
      "----- 3500th batch | Train Acc: 88.2188%\n",
      "----- 3600th batch | Train Acc: 87.9062%\n",
      "----- 3700th batch | Train Acc: 89.9688%\n",
      "----- 3800th batch | Train Acc: 88.9062%\n",
      "----- 3900th batch | Train Acc: 89.0000%\n",
      "Epoch: 2 | Training Loss: 0.3943 | Val Loss: 0.2971 | Train Acc: 87.66% | Val Acc: 90.08%\n",
      "----- 0100th batch | Train Acc: 89.5938%\n",
      "----- 0200th batch | Train Acc: 88.9062%\n",
      "----- 0300th batch | Train Acc: 90.0312%\n",
      "----- 0400th batch | Train Acc: 89.0000%\n",
      "----- 0500th batch | Train Acc: 89.3125%\n",
      "----- 0600th batch | Train Acc: 88.6875%\n",
      "----- 0700th batch | Train Acc: 89.0938%\n",
      "----- 0800th batch | Train Acc: 89.4375%\n",
      "----- 0900th batch | Train Acc: 89.8750%\n",
      "----- 1000th batch | Train Acc: 89.8750%\n",
      "----- 1100th batch | Train Acc: 88.9062%\n",
      "----- 1200th batch | Train Acc: 89.4688%\n",
      "----- 1300th batch | Train Acc: 89.5625%\n",
      "----- 1400th batch | Train Acc: 89.4688%\n",
      "----- 1500th batch | Train Acc: 90.3438%\n",
      "----- 1600th batch | Train Acc: 90.2188%\n",
      "----- 1700th batch | Train Acc: 90.4062%\n",
      "----- 1800th batch | Train Acc: 89.6562%\n",
      "----- 1900th batch | Train Acc: 89.8750%\n",
      "----- 2000th batch | Train Acc: 89.8750%\n",
      "----- 2100th batch | Train Acc: 89.6562%\n",
      "----- 2200th batch | Train Acc: 90.9375%\n",
      "----- 2300th batch | Train Acc: 90.3125%\n",
      "----- 2400th batch | Train Acc: 89.5938%\n",
      "----- 2500th batch | Train Acc: 89.4375%\n",
      "----- 2600th batch | Train Acc: 89.6875%\n",
      "----- 2700th batch | Train Acc: 89.7188%\n",
      "----- 2800th batch | Train Acc: 89.1250%\n",
      "----- 2900th batch | Train Acc: 89.8438%\n",
      "----- 3000th batch | Train Acc: 90.2188%\n",
      "----- 3100th batch | Train Acc: 90.4375%\n",
      "----- 3200th batch | Train Acc: 89.5625%\n",
      "----- 3300th batch | Train Acc: 89.6250%\n",
      "----- 3400th batch | Train Acc: 90.5000%\n",
      "----- 3500th batch | Train Acc: 90.6562%\n",
      "----- 3600th batch | Train Acc: 90.5938%\n",
      "----- 3700th batch | Train Acc: 90.5625%\n",
      "----- 3800th batch | Train Acc: 89.0312%\n",
      "----- 3900th batch | Train Acc: 89.7500%\n",
      "Epoch: 3 | Training Loss: 0.3148 | Val Loss: 0.2239 | Train Acc: 89.77% | Val Acc: 92.85%\n",
      "----- 0100th batch | Train Acc: 91.1250%\n",
      "----- 0200th batch | Train Acc: 90.2188%\n",
      "----- 0300th batch | Train Acc: 90.7812%\n",
      "----- 0400th batch | Train Acc: 90.5938%\n",
      "----- 0500th batch | Train Acc: 91.6250%\n",
      "----- 0600th batch | Train Acc: 91.2500%\n",
      "----- 0700th batch | Train Acc: 91.1562%\n",
      "----- 0800th batch | Train Acc: 90.8438%\n",
      "----- 0900th batch | Train Acc: 90.8750%\n",
      "----- 1000th batch | Train Acc: 91.0938%\n",
      "----- 1100th batch | Train Acc: 90.8125%\n",
      "----- 1200th batch | Train Acc: 90.9375%\n",
      "----- 1300th batch | Train Acc: 90.3125%\n",
      "----- 1400th batch | Train Acc: 90.9375%\n",
      "----- 1500th batch | Train Acc: 91.0000%\n",
      "----- 1600th batch | Train Acc: 90.2500%\n",
      "----- 1700th batch | Train Acc: 91.8125%\n",
      "----- 1800th batch | Train Acc: 90.8750%\n",
      "----- 1900th batch | Train Acc: 91.9688%\n",
      "----- 2000th batch | Train Acc: 90.7500%\n",
      "----- 2100th batch | Train Acc: 91.9375%\n",
      "----- 2200th batch | Train Acc: 90.6250%\n",
      "----- 2300th batch | Train Acc: 91.0938%\n",
      "----- 2400th batch | Train Acc: 91.3125%\n",
      "----- 2500th batch | Train Acc: 91.1875%\n",
      "----- 2600th batch | Train Acc: 91.5000%\n",
      "----- 2700th batch | Train Acc: 90.4688%\n",
      "----- 2800th batch | Train Acc: 92.6562%\n",
      "----- 2900th batch | Train Acc: 92.3125%\n",
      "----- 3000th batch | Train Acc: 90.3438%\n",
      "----- 3100th batch | Train Acc: 90.5000%\n",
      "----- 3200th batch | Train Acc: 91.4062%\n",
      "----- 3300th batch | Train Acc: 91.6250%\n",
      "----- 3400th batch | Train Acc: 92.0938%\n",
      "----- 3500th batch | Train Acc: 91.5625%\n",
      "----- 3600th batch | Train Acc: 91.7500%\n",
      "----- 3700th batch | Train Acc: 91.5312%\n",
      "----- 3800th batch | Train Acc: 92.2812%\n",
      "----- 3900th batch | Train Acc: 91.2188%\n",
      "Epoch: 4 | Training Loss: 0.2701 | Val Loss: 0.2112 | Train Acc: 91.20% | Val Acc: 92.88%\n",
      "----- 0100th batch | Train Acc: 92.1875%\n",
      "----- 0200th batch | Train Acc: 91.1250%\n",
      "----- 0300th batch | Train Acc: 90.8438%\n",
      "----- 0400th batch | Train Acc: 91.5312%\n",
      "----- 0500th batch | Train Acc: 91.1562%\n",
      "----- 0600th batch | Train Acc: 91.8750%\n",
      "----- 0700th batch | Train Acc: 91.9062%\n",
      "----- 0800th batch | Train Acc: 92.7812%\n",
      "----- 0900th batch | Train Acc: 91.7188%\n",
      "----- 1000th batch | Train Acc: 92.4062%\n",
      "----- 1100th batch | Train Acc: 91.4688%\n",
      "----- 1200th batch | Train Acc: 91.4688%\n",
      "----- 1300th batch | Train Acc: 91.7188%\n",
      "----- 1400th batch | Train Acc: 92.2812%\n",
      "----- 1500th batch | Train Acc: 92.0938%\n",
      "----- 1600th batch | Train Acc: 91.2500%\n",
      "----- 1700th batch | Train Acc: 91.7812%\n",
      "----- 1800th batch | Train Acc: 92.5938%\n",
      "----- 1900th batch | Train Acc: 91.8125%\n",
      "----- 2000th batch | Train Acc: 91.5625%\n",
      "----- 2100th batch | Train Acc: 91.2500%\n",
      "----- 2200th batch | Train Acc: 92.4062%\n",
      "----- 2300th batch | Train Acc: 91.4062%\n",
      "----- 2400th batch | Train Acc: 92.0000%\n",
      "----- 2500th batch | Train Acc: 91.5000%\n",
      "----- 2600th batch | Train Acc: 92.0312%\n",
      "----- 2700th batch | Train Acc: 92.8438%\n",
      "----- 2800th batch | Train Acc: 92.4688%\n",
      "----- 2900th batch | Train Acc: 92.4062%\n",
      "----- 3000th batch | Train Acc: 92.3750%\n",
      "----- 3100th batch | Train Acc: 92.7188%\n",
      "----- 3200th batch | Train Acc: 91.4688%\n",
      "----- 3300th batch | Train Acc: 91.1875%\n",
      "----- 3400th batch | Train Acc: 92.2500%\n",
      "----- 3500th batch | Train Acc: 91.2812%\n",
      "----- 3600th batch | Train Acc: 92.1562%\n",
      "----- 3700th batch | Train Acc: 92.1562%\n",
      "----- 3800th batch | Train Acc: 91.9375%\n",
      "----- 3900th batch | Train Acc: 91.5625%\n",
      "Epoch: 5 | Training Loss: 0.2452 | Val Loss: 0.2164 | Train Acc: 91.87% | Val Acc: 93.09%\n",
      "----- 0100th batch | Train Acc: 91.9375%\n",
      "----- 0200th batch | Train Acc: 92.6875%\n",
      "----- 0300th batch | Train Acc: 93.1250%\n",
      "----- 0400th batch | Train Acc: 92.1250%\n",
      "----- 0500th batch | Train Acc: 91.4375%\n",
      "----- 0600th batch | Train Acc: 92.9688%\n",
      "----- 0700th batch | Train Acc: 92.1562%\n",
      "----- 0800th batch | Train Acc: 92.4688%\n",
      "----- 0900th batch | Train Acc: 91.3125%\n",
      "----- 1000th batch | Train Acc: 92.2812%\n",
      "----- 1100th batch | Train Acc: 91.7500%\n",
      "----- 1200th batch | Train Acc: 92.6250%\n",
      "----- 1300th batch | Train Acc: 91.7812%\n",
      "----- 1400th batch | Train Acc: 91.7500%\n",
      "----- 1500th batch | Train Acc: 92.9062%\n",
      "----- 1600th batch | Train Acc: 91.4688%\n",
      "----- 1700th batch | Train Acc: 91.7188%\n",
      "----- 1800th batch | Train Acc: 92.6250%\n",
      "----- 1900th batch | Train Acc: 92.1562%\n",
      "----- 2000th batch | Train Acc: 91.9375%\n",
      "----- 2100th batch | Train Acc: 93.7188%\n",
      "----- 2200th batch | Train Acc: 92.1875%\n",
      "----- 2300th batch | Train Acc: 92.0625%\n",
      "----- 2400th batch | Train Acc: 92.5000%\n",
      "----- 2500th batch | Train Acc: 92.3438%\n",
      "----- 2600th batch | Train Acc: 92.9062%\n",
      "----- 2700th batch | Train Acc: 92.9375%\n",
      "----- 2800th batch | Train Acc: 92.2188%\n",
      "----- 2900th batch | Train Acc: 92.3750%\n",
      "----- 3000th batch | Train Acc: 92.6875%\n",
      "----- 3100th batch | Train Acc: 92.7812%\n",
      "----- 3200th batch | Train Acc: 92.8750%\n",
      "----- 3300th batch | Train Acc: 92.7500%\n",
      "----- 3400th batch | Train Acc: 92.5000%\n",
      "----- 3500th batch | Train Acc: 92.3125%\n",
      "----- 3600th batch | Train Acc: 92.7500%\n",
      "----- 3700th batch | Train Acc: 92.2812%\n",
      "----- 3800th batch | Train Acc: 92.2188%\n",
      "----- 3900th batch | Train Acc: 92.8438%\n",
      "Epoch: 6 | Training Loss: 0.2290 | Val Loss: 0.1888 | Train Acc: 92.37% | Val Acc: 93.54%\n",
      "----- 0100th batch | Train Acc: 93.0625%\n",
      "----- 0200th batch | Train Acc: 92.9375%\n",
      "----- 0300th batch | Train Acc: 92.4375%\n",
      "----- 0400th batch | Train Acc: 92.7812%\n",
      "----- 0500th batch | Train Acc: 92.8750%\n",
      "----- 0600th batch | Train Acc: 92.9062%\n",
      "----- 0700th batch | Train Acc: 92.6562%\n",
      "----- 0800th batch | Train Acc: 93.1250%\n",
      "----- 0900th batch | Train Acc: 93.1250%\n",
      "----- 1000th batch | Train Acc: 92.1562%\n",
      "----- 1100th batch | Train Acc: 92.9062%\n",
      "----- 1200th batch | Train Acc: 92.9375%\n",
      "----- 1300th batch | Train Acc: 91.6250%\n",
      "----- 1400th batch | Train Acc: 92.2812%\n",
      "----- 1500th batch | Train Acc: 92.8438%\n",
      "----- 1600th batch | Train Acc: 92.5625%\n",
      "----- 1700th batch | Train Acc: 92.8438%\n",
      "----- 1800th batch | Train Acc: 93.0938%\n",
      "----- 1900th batch | Train Acc: 92.1250%\n",
      "----- 2000th batch | Train Acc: 91.8438%\n",
      "----- 2100th batch | Train Acc: 92.3750%\n",
      "----- 2200th batch | Train Acc: 93.2812%\n",
      "----- 2300th batch | Train Acc: 92.7500%\n",
      "----- 2400th batch | Train Acc: 92.4375%\n",
      "----- 2500th batch | Train Acc: 92.5938%\n",
      "----- 2600th batch | Train Acc: 93.5938%\n",
      "----- 2700th batch | Train Acc: 93.0312%\n",
      "----- 2800th batch | Train Acc: 92.8125%\n",
      "----- 2900th batch | Train Acc: 93.4375%\n",
      "----- 3000th batch | Train Acc: 92.4062%\n",
      "----- 3100th batch | Train Acc: 93.1250%\n",
      "----- 3200th batch | Train Acc: 92.6562%\n",
      "----- 3300th batch | Train Acc: 93.1875%\n",
      "----- 3400th batch | Train Acc: 93.0312%\n",
      "----- 3500th batch | Train Acc: 92.5938%\n",
      "----- 3600th batch | Train Acc: 93.0000%\n",
      "----- 3700th batch | Train Acc: 91.5000%\n",
      "----- 3800th batch | Train Acc: 92.8750%\n",
      "----- 3900th batch | Train Acc: 93.0312%\n",
      "Epoch: 7 | Training Loss: 0.2145 | Val Loss: 0.1783 | Train Acc: 92.74% | Val Acc: 94.13%\n",
      "----- 0100th batch | Train Acc: 93.8750%\n",
      "----- 0200th batch | Train Acc: 93.2500%\n",
      "----- 0300th batch | Train Acc: 94.2812%\n",
      "----- 0400th batch | Train Acc: 92.3750%\n",
      "----- 0500th batch | Train Acc: 93.0625%\n",
      "----- 0600th batch | Train Acc: 92.6562%\n",
      "----- 0700th batch | Train Acc: 94.2812%\n",
      "----- 0800th batch | Train Acc: 92.8750%\n",
      "----- 0900th batch | Train Acc: 93.5938%\n",
      "----- 1000th batch | Train Acc: 93.6875%\n",
      "----- 1100th batch | Train Acc: 93.0312%\n",
      "----- 1200th batch | Train Acc: 92.7188%\n",
      "----- 1300th batch | Train Acc: 93.3438%\n",
      "----- 1400th batch | Train Acc: 92.9062%\n",
      "----- 1500th batch | Train Acc: 93.2812%\n",
      "----- 1600th batch | Train Acc: 92.7188%\n",
      "----- 1700th batch | Train Acc: 92.1875%\n",
      "----- 1800th batch | Train Acc: 93.1250%\n",
      "----- 1900th batch | Train Acc: 93.1250%\n",
      "----- 2000th batch | Train Acc: 93.6250%\n",
      "----- 2100th batch | Train Acc: 93.7812%\n",
      "----- 2200th batch | Train Acc: 93.6562%\n",
      "----- 2300th batch | Train Acc: 93.5625%\n",
      "----- 2400th batch | Train Acc: 94.0000%\n",
      "----- 2500th batch | Train Acc: 92.6875%\n",
      "----- 2600th batch | Train Acc: 93.0312%\n",
      "----- 2700th batch | Train Acc: 92.9688%\n",
      "----- 2800th batch | Train Acc: 93.5000%\n",
      "----- 2900th batch | Train Acc: 93.2188%\n",
      "----- 3000th batch | Train Acc: 93.2500%\n",
      "----- 3100th batch | Train Acc: 93.7812%\n",
      "----- 3200th batch | Train Acc: 93.1562%\n",
      "----- 3300th batch | Train Acc: 93.5000%\n",
      "----- 3400th batch | Train Acc: 92.2812%\n",
      "----- 3500th batch | Train Acc: 93.0938%\n",
      "----- 3600th batch | Train Acc: 93.5000%\n",
      "----- 3700th batch | Train Acc: 92.3125%\n",
      "----- 3800th batch | Train Acc: 92.9062%\n",
      "----- 3900th batch | Train Acc: 93.3125%\n",
      "Epoch: 8 | Training Loss: 0.2024 | Val Loss: 0.1792 | Train Acc: 93.22% | Val Acc: 93.99%\n",
      "EarlyStopping counter: 1 out of 3\n",
      "----- 0100th batch | Train Acc: 93.5625%\n",
      "----- 0200th batch | Train Acc: 93.0312%\n",
      "----- 0300th batch | Train Acc: 93.2812%\n",
      "----- 0400th batch | Train Acc: 93.7188%\n",
      "----- 0500th batch | Train Acc: 93.7188%\n",
      "----- 0600th batch | Train Acc: 92.6562%\n",
      "----- 0700th batch | Train Acc: 92.9062%\n",
      "----- 0800th batch | Train Acc: 93.2188%\n",
      "----- 0900th batch | Train Acc: 93.8125%\n",
      "----- 1000th batch | Train Acc: 92.9062%\n",
      "----- 1100th batch | Train Acc: 93.6562%\n",
      "----- 1200th batch | Train Acc: 94.1562%\n",
      "----- 1300th batch | Train Acc: 93.2500%\n",
      "----- 1400th batch | Train Acc: 93.9062%\n",
      "----- 1500th batch | Train Acc: 93.0938%\n",
      "----- 1600th batch | Train Acc: 93.5000%\n",
      "----- 1700th batch | Train Acc: 92.8750%\n",
      "----- 1800th batch | Train Acc: 93.3750%\n",
      "----- 1900th batch | Train Acc: 93.7188%\n",
      "----- 2000th batch | Train Acc: 92.4062%\n",
      "----- 2100th batch | Train Acc: 93.6250%\n",
      "----- 2200th batch | Train Acc: 93.7500%\n",
      "----- 2300th batch | Train Acc: 93.3750%\n",
      "----- 2400th batch | Train Acc: 92.8438%\n",
      "----- 2500th batch | Train Acc: 93.3750%\n",
      "----- 2600th batch | Train Acc: 94.4375%\n",
      "----- 2700th batch | Train Acc: 93.1562%\n",
      "----- 2800th batch | Train Acc: 93.6875%\n",
      "----- 2900th batch | Train Acc: 92.7188%\n",
      "----- 3000th batch | Train Acc: 93.3750%\n",
      "----- 3100th batch | Train Acc: 93.2812%\n",
      "----- 3200th batch | Train Acc: 92.2500%\n",
      "----- 3300th batch | Train Acc: 93.1875%\n",
      "----- 3400th batch | Train Acc: 92.3750%\n",
      "----- 3500th batch | Train Acc: 93.6250%\n",
      "----- 3600th batch | Train Acc: 93.9375%\n",
      "----- 3700th batch | Train Acc: 93.7188%\n",
      "----- 3800th batch | Train Acc: 94.1250%\n",
      "----- 3900th batch | Train Acc: 93.1562%\n",
      "Epoch: 9 | Training Loss: 0.1957 | Val Loss: 0.1768 | Train Acc: 93.35% | Val Acc: 94.00%\n",
      "EarlyStopping counter: 2 out of 3\n",
      "----- 0100th batch | Train Acc: 93.5000%\n",
      "----- 0200th batch | Train Acc: 93.6250%\n",
      "----- 0300th batch | Train Acc: 93.2812%\n",
      "----- 0400th batch | Train Acc: 93.9375%\n",
      "----- 0500th batch | Train Acc: 93.5000%\n",
      "----- 0600th batch | Train Acc: 94.2812%\n",
      "----- 0700th batch | Train Acc: 93.7812%\n",
      "----- 0800th batch | Train Acc: 93.6875%\n",
      "----- 0900th batch | Train Acc: 93.8125%\n",
      "----- 1000th batch | Train Acc: 94.4062%\n",
      "----- 1100th batch | Train Acc: 93.8750%\n",
      "----- 1200th batch | Train Acc: 93.8125%\n",
      "----- 1300th batch | Train Acc: 94.2188%\n",
      "----- 1400th batch | Train Acc: 93.1562%\n",
      "----- 1500th batch | Train Acc: 93.2500%\n",
      "----- 1600th batch | Train Acc: 93.7188%\n",
      "----- 1700th batch | Train Acc: 93.6875%\n",
      "----- 1800th batch | Train Acc: 93.4375%\n",
      "----- 1900th batch | Train Acc: 93.1875%\n",
      "----- 2000th batch | Train Acc: 92.6875%\n",
      "----- 2100th batch | Train Acc: 93.6562%\n",
      "----- 2200th batch | Train Acc: 93.0938%\n",
      "----- 2300th batch | Train Acc: 93.3125%\n",
      "----- 2400th batch | Train Acc: 93.9062%\n",
      "----- 2500th batch | Train Acc: 93.3750%\n",
      "----- 2600th batch | Train Acc: 93.4688%\n",
      "----- 2700th batch | Train Acc: 93.6875%\n",
      "----- 2800th batch | Train Acc: 93.8438%\n",
      "----- 2900th batch | Train Acc: 94.6250%\n",
      "----- 3000th batch | Train Acc: 93.4375%\n",
      "----- 3100th batch | Train Acc: 93.6562%\n",
      "----- 3200th batch | Train Acc: 93.4375%\n",
      "----- 3300th batch | Train Acc: 93.7812%\n",
      "----- 3400th batch | Train Acc: 93.5938%\n",
      "----- 3500th batch | Train Acc: 93.1875%\n",
      "----- 3600th batch | Train Acc: 93.0938%\n",
      "----- 3700th batch | Train Acc: 93.6875%\n",
      "----- 3800th batch | Train Acc: 93.5938%\n",
      "----- 3900th batch | Train Acc: 93.2188%\n",
      "Epoch: 10 | Training Loss: 0.1880 | Val Loss: 0.1936 | Train Acc: 93.60% | Val Acc: 93.61%\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping condition met --- TRAINING STOPPED\n"
     ]
    }
   ],
   "source": [
    "model_fit = train_model(model, epochs, learning_rate, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PretrainedModel(\n",
       "  (block): Sequential(\n",
       "    (0): Conv2d(1, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (1): Linear(in_features=256, out_features=26, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet.fc = nn.Linear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "dacon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
